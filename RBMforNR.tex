%\input{ThesisPreamble}
%\begin{document}

\chapter{Model Reduction for Compact Binary Coalescence}

Numerical methods are used to solve PDEs, however for complicated problems the number of parameters required may be quite large, resulting in prohibitive computational cost. The reduced basis method is a means of reducing the number of degrees of freedom in order to reduce costs and make solving the problem feasible. Sometimes we are interested in the field that is the solution to the PDE, perhaps a termperature function. Other times we are intersted in a function of this field, perhaps the maximum temperature. This method evaluates the relationship between the input parameters and the output function, hopefully bypassing many of the expensive computations. This is useful in many real-time and multiple query contexts.

Consider a parameterised model $\Mcal:\Pcal \rightarrow \Ucal \rightarrow \mathcal{Y}$ of some phenomenon, where given a parameter value $\pbf$ in the parameter space $\Pcal\subset\R^N$, the model predicts a state $\bfu(\bfp)$ in the state space $\mathcal{U}\subset\R^K$ from which we can extract output  $\ybf(\pbf)\in \mathcal{Y}\subset\R^M$. Suppose in an experimental setting we obtain noisy observations $\ybf_d\in \mathcal{Y}$. The inverse problem is to estimate $\pbf$ given $\ybf_d$, a problem that is often ill-posed. 

In order to decrease the computational load required we reduce the parameter space from $\R^N$ to $\R^{N_r}$ as any exploration or computation in a lower dimensional space is cheaper. 
For this we generate a sequence of parameter vectors $\pbf_1, \pbf_2,\ldots$ in $\R^N$ which are defined using a greedy algorithm 
\begin{equation}\label{eq:greedyOpt}
\pbf_{k+1} = \argmax_{\pbf\in\R^N} \left(\frac{1}{2}\|\ybf(\pbf)-\ybf(Q_k \pbf)\|^2 + \beta \log \pi(\pbf)\right),
\end{equation}
where the $Q_k:\R^N\rightarrow \R^N$ are orthogonal projections with 
$\range(Q_k)=\span\{\pbf_1,\ldots,\pbf_k\}$ for $k\geq 1$ and $Q_0:=0$. 
This choice introduces the new $\pbf_{k+1}$ to make sure that a large number of observations $\ybf_d$ can be approximated in the range of $Q_{k+1}$ while exploring parameter vectors which are sufficiently likely according to the prior. This method is discussed in detail in \cite{lieberman2009}parameter.
%
%In this paper we will demonstrate the Reduced Basis Method for solving a problem of the form 
%\begin{equation}
%\Mcal:\Pcal \rightarrow \Ucal_0 \rightarrow \Ucal_1 \rightarrow \mathcal{Y},
%\end{equation}


\section{One dimensional wire}
Consider a simple one dimensional case. We have a wire fixed at ends $x=0$ and $x=1$, under constant tension $T=1$, and subject to a transverse load $f(x;p)$, where $p \in \mathcal{P}$ is our parameter. Let $u(x,t;p)$ denote the transverse displacement of the wire at point $x$ and time $t$, with boundary condition $u(0,t;p)=u(1,t;p)=0$. We find the initial condition $u_0(x;p)=u(x,0;p)\in H_0[0,1]$ by solving the one dimensional Poisson equation
\begin{equation}\label{eq:1dConstraint}
-\frac{\pd^2 u_0}{\pd x^2}(x) = f(x;p). 
\end{equation}
The system time evolution is governed by the one dimensional wave equation, where the wire is assumed to be released from rest
\begin{equation}\label{eq:1dEvolution}
\frac{\pd^2 u}{\pd t^2}=c^2 \frac{\pd^2 u}{\pd x^2}
\end{equation}
The observations $\mathbf{y}_i \in \Rbb^j$ are a set of time series vectors of the displacement amplitudes at some points $x_i \in [0,1]$ at $j$ sample times. 

\subsection{Exact Solution}
In this example we can explicitly compute the operator $\Mcal$. The solution to (\ref{eq:1dConstraint}) is 
\begin{equation}\label{eq:1DInitCond}
u_0(x;p)=\int_0^1 G(x,\xi) f(\xi;p) d \xi,
\end{equation}
Where $G(x,\xi)$ is Green's function given by
\begin{equation}
\label{eq:Greens}
G(x,\xi)=\begin{cases} \xi(1-x) \mbox{ if } 0\leq \xi \leq x \\
					  x(1-\xi) \mbox{ if } x \leq \xi \leq 1.
		\end{cases}
\end{equation}
The general solution to (\ref{eq:1dEvolution}) is
\begin{equation}\label{eq:1Ddisplacements}
u(x,t;p)=\sum^{\infty}_{n=1} \beta_n(p)\sin(n\pi x)\cos(c n \pi t). 
\end{equation}
We find by substituting $t=0$ that the $\beta_n$ are the Fourier sine series coefficients of $u_0$:
\begin{equation}
\beta_n(p)=2\int_0^1 u_0(x;p) \sin(n\pi x) dx.
\end{equation}
The output $\mathbf{y}$ is found by substituting the observation points $\bfx$ into (\ref{eq:1DInitCond}):
\begin{equation}\label{eq:1Doutput}
\bfy(t)=\sum^{\infty}_{n=1} \beta_n(p)\cos(c n \pi t)\sin(\bfx n\pi).
\end{equation}


\subsection{Numerical Solution}
We now develop the operator $\Mcal$ for the numerical solution to the system.

\subsubsection{Initial condition}
Consider a finite differencing scheme. Our domain $[0,1]$ is divided into $n$ subintervals of length $h=\Delta x = 1 /n$ so that the endpoints of the intervals are given by $x_j = j  \Delta x = jh$ for $j= 0, 1, \cdots n$. Using a Taylor expansion of $u$ around $x$ we find
\begin{equation}
\begin{array}{lcl}
-u_{xx}(x)=\frac{-u(x+h)+2u(x)-u(x-h)}{h^2}&=& f(x)  \mbox{ by eq \ref{eq:1dConstraint}} \\
\rightarrow \frac{-u_{j+1} + 2u_j - u_{j-1} }{h^2} = f(x_j)
\end{array}
\end{equation}
where $u_i = u(x_i)$. By setting the endpoints according to our boundary conditions $u_0 = u_n=0$ and defining
\begin{equation}
\hat{u}=[u_1,...,u_{n-1}]^T \mbox{ , } \hat{f} = [f(x_1),...,f(x_{n-1})], A= \left[\begin{array}{ccccc}
2  & -1 & 0  & \cdots & 0\\
-1 & 2  & -1 &  & \vdots \\
0  & -1 & 2  & \ddots & 0 \\
\vdots & & \ddots & \ddots & -1 \\
0 & \cdots & 0 & -1 & 2 
\end{array}\right]
\end{equation}
the initial value problem then becomes finding the solution to the matrix eq
\begin{equation}
A \hat{u} = h^2 \hat{f}
\end{equation}
the solution of which is 
\begin{equation}
\hat{u}_0(p) = A^{-1}h^2\hat{f}(p)
\end{equation}

\subsubsection{Evolution}
Defining $\hat{u}(x,t)$ similarly to above, \ref{eq:1dEvolution} becomes
\begin{equation}
\frac{d^2 \hat{u}}{dt^2}+c^2 A \hat{u}=0
\end{equation}
with initial conditions $\hat{u}(0,p)=\hat{u}_0(p)$ and $\hat{u}'(0,p)=\hat{u}_1(p)$. This admits the solution
\begin{equation}\label{eq:dispSoln}
\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)A^{-1}\hat{f}(p)
\end{equation}

%\subsection{results for different fs}
%\subsubsection{$f(x)= \sin(p \pi x)$}
%$f(x)= \sin(p \pi x)$ where $p \in \mathcal{P} \subset \mathbb{R}_+$ is the spatial frequency of the load. We take the wave speed $c=1$. 
%The initial condition $u_0(x)=\frac{1}{\pi^2 p^2}\sin(\pi p x)$, so we see that the spatial frequency and the amplitude of the initial condition depend on the parameter $p$. Likewise, the full solution $u(x,t)=\frac{1}{\pi^2 p^2}\sin(\pi p x)\cos(\pi p c t)$. The observations $\mathbf{y}$ are given by
%
%\begin{equation}
%\mathbf{y}=\left[
%\begin{array}{c}
%u(0.1,t)\\
%u(0.2,t)\\
%u(0.4,t)\\
%u(0.8,t)\\
%\end{array}
%\right]
%= \frac{\cos(\pi p c t)}{\pi^2 p^2} \left[
%\begin{array}{c}
%\sin(0.1 \pi p) \\
%\sin(0.2 \pi p) \\
%\sin(0.4 \pi p) \\
%\sin(0.8 \pi p) \\
%\end{array}
%\right]
%\end{equation}

%\subsection{Experiment 1}
%To start, let us consider just the evolution equation \ref{eq:1dEvolution}. Take the parameter to be simply the initial displacements at the internal gridpoints $\hat{\bfx}=[h,2h,\cdots, Nh]^T$, $h=1/(N+1)$. 
%\begin{equation}
%\label{eq:InitCondNew}
%\bfp = \hat{u_0}(\hat{\bfx})
%\end{equation}
%This means that (\ref{eq:dispSoln}) becomes
%\begin{equation}
%\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)\bfp
%\end{equation}
%Let us take the output to be the displacement over times $t_1,\cdots, t_m$ just at one point $x_s$. 
%\begin{equation}
%\bfy=\left[
%\begin{array}{c}
%\bfe_s^T \cos(cA^{\frac{1}{2}}t_1) \\
%\vdots\\
%\bfe_s^T \cos(cA^{\frac{1}{2}}t_m)
%\end{array}
%\right]\bfp \equiv M \bfp
%\end{equation}
%where $\bfe_s=[0,\dots,1\dots,0]$ with the $1$ in the $s^{th}$ position. We investigate how well we are able to approximate the output $\bfy(\bfp)$ by reduced models $\bfy(Q_k \bfp)$ using $N=[5,10,20,40]$ when the initial displacement is given by
%\begin{equation}
%u_0(x) = \sum_{k=1}^5 c_k \sin(2\pi k x)
%\end{equation}
%where the $c_k$ are uniformly distributed in $[0,1]$. The parameter space $\mathcal{P}={\bfp \in \mathbb{R}^N | \| \bfp \| = 1}$. 
%
%We define approximations to the parameter vectors $\bfp_k$ defined in equation~(\ref{eq:greedyOpt}) by
%\begin{equation}
%\bfp_{k+1} = \argmax_{\bfp\in \Mcal_k }\frac{1}{2}\|\bfy(\bfp)-\bfy(Q_k \bfp)\|^2 .
%\end{equation}
%where each $\Mcal_k$ contains $m$ independent samples of the uniform distribution over $\Pcal$ and the the elements of $\Mcal_k$ and $\Mcal_j$ are independent for $j\neq k$. In the experiment we chose $m=100$. Note that this allows us to drop the $\beta \log \pi(\bfp)$ term that appears in equation (\ref{eq:greedyOpt}). We denote the approximations of $\bfp_k$ by the same symbol $\bfp_k$ for simplicity.
%
%To quantify how well the reduced model approximation $\ybf(Q_k\pbf)$ can recover the full model $\ybf(\pbf)$ for arbitrary $\pbf$, we use the relative $L_2$ norm 
%$$\frac{\sqrt{\int_\Pcal ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2\, d\omega}}{\sqrt{\int_\Pcal ||\ybf(\pbf)||^2\, d\omega}},$$ 
%where $\omega$ is a probability measure over $\Pcal$.
%We approximate this relative root mean squared error by Monte Carlo quadrature as
%\begin{equation}\label{eq:MCrmse}
%\rho_k = \frac{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2}}{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)||^2}}.
%\end{equation}
%
%\begin{figure}[t]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/Exp1results/rmsevsndim.pdf}
%  \caption{Relative root mean square error calculted using equation \ref{eq:MCrmse} }\label{fig:Exp1rmsevecs}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/Exp1results/RBvecsreqforeps.pdf}
%  \caption{Number of basis vectors required to achieve relative error less than $\epsilon=1\times 10^{-10}$}
%\endminipage
%
%\end{figure}
%
%Figure \ref{fig:Exp1rmsevecs} shows $\rho_k$ vs $k$ for $N=5,10,20,40$. For $N=5$ only two basis vectors are required to achieve accuracy within $\epsilon=1\times 10^{-10}$. This is due to the symmetry of the function about $x_2=0.5$, with $u(x_0)=-u(x_4)$ and $u(x_1)=-u(x_3)$ leaving just two independent components. Four basis vectors are required for $N=10,20,40$ which reflects the four basis functions used to construct $u_0$.

\subsection{Reduced basis construction}
To start, let us consider just the evolution equation (\ref{eq:1dEvolution}). We construct the initial condition as follows
\begin{equation}
\label{eq:Exp2initcond}
u_0 = \sum_{n=1}^N p_n \sin(2\pi n x)/n^2,
\end{equation}
where the parameter $\bfp=[p_n]$ and (\ref{eq:dispSoln}) becomes
\begin{equation}
\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)\left[ \begin{array}{ccc}
\sin(2\pi \bfx) & \cdots & \sin(2N \pi \bfx)/N^2 
\end{array}
\right]\bfp.
\end{equation}

The number of gridpoints is fixed at $n=100$. Let us take the output to be the displacement over times $t_1,\cdots, t_m$ just at one gridpoint $x_s$. 
\begin{equation}
\bfy=\left[
\begin{array}{ccc}
\bfe_s^T \cos(cA^{\frac{1}{2}}t_1)\sin(2\pi \bfx) & \cdots & \bfe_s^T \cos(cA^{\frac{1}{2}}t_1)\sin(2N \pi \bfx)/N^2 \\
\vdots & \ddots & \vdots \\
\bfe_s^T \cos(cA^{\frac{1}{2}}t_m)\sin(2\pi \bfx) & \cdots & \bfe_s^T \cos(cA^{\frac{1}{2}}t_m)\sin(2N \pi \bfx)/N^2
\end{array}
\right]\bfp \equiv M \bfp
\end{equation}
where $\bfe_s=[0,\dots,1\dots,0]$ with the $1$ in the $s^{th}$ position. 

We investigate how well we are able to approximate the output $\bfy(\bfp)$ by reduced models $\bfy(Q_k \bfp)$. %
%
%The system matrix is $A=MB$ where $M$ is as above and 
%\begin{equation}
%B=\left[ \begin{array}{ccc}
%\sin(2\pi \bfx) & \cdots & \sin(2N \pi \bfx)/N^2 
%\end{array}
%\right]
%\end{equation}
We define approximations to the parameter vectors $\bfp_k$ defined in equation (\ref{eq:greedyOpt}) by
\begin{equation}
\bfp_{k+1} = \argmax_{\bfp\in \Mcal_k }\frac{1}{2}\|\bfy(\bfp)-\bfy(Q_k \bfp)\|^2 .
\end{equation}
where each $\Mcal_k$ contains $m$ independent samples of the uniform distribution over $\Pcal$ and the the elements of $\Mcal_k$ and $\Mcal_j$ are independent for $j\neq k$.  Note that this allows us to drop the $\beta \log \pi(\bfp)$ term that appears in equation (\ref{eq:greedyOpt}). We denote the approximations of $\bfp_k$ by the same symbol $\bfp_k$ for simplicity. Here $ |\Mcal_k| = 100$, and $Q_k = P_k (P_k^T P_k)^{-1} P_k^T$ where 
\[ P_k = [ \bfp_1 \cdots \bfp_k ]. \] 

To quantify how well the reduced model approximation $\ybf(Q_k\pbf)$ can recover the full model $\ybf(\pbf)$ for arbitrary $\pbf$, we use the relative $L_2$ norm 
$$\frac{\sqrt{\int_\Pcal ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2\, d\omega}}{\sqrt{\int_\Pcal ||\ybf(\pbf)||^2\, d\omega}},$$ 
where $\omega$ is a probability measure over $\Pcal$.
We approximate this relative root mean squared error by Monte Carlo quadrature as
\begin{equation}\label{eq:MCrmse}
\rho_k = \frac{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2}}{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)||^2}}.
\end{equation}

Reduced bases were constructed for full basis dimensions $N=5, 10, 15, 20,$ $25, 30, 40,$ $50,$ $60,$ $70,$ $80, 90, 100$. Figure \ref{fig:Exp2rmsevecs} shows $\rho_k$ vs $k$ for each $N$, calculated using (\ref{eq:MCrmse}), and figure \ref{fig:Exp2vecsreq} shows the number of reduced basis vectors required to achieve $\rho_k < 0.05$. This shows that for more complex problems (a higher $N$), more reduced basis functions are required to achieve the error threshold, but the relative number of basis functions required decreases. In the case of $N=100$, a reduced model error of $\rho_k<0.05$ is achieved with $48$ basis vectors, a reduction in the parameter space dimension of more than half.

%\begin{figure}[h]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/rmsevsndim.pdf}
%  \caption{Relative root mean square error calculted using equation (\ref{eq:MCrmse}) }\label{fig:Exp2rmsevecs}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/RBvecsreqforeps.pdf}
%  \caption{Number of basis vectors required to achieve relative error less than $\rho_k=0.05$}\label{fig:Exp2vecsreq}
%\endminipage
%\end{figure}

\subsubsection{Inverse Experiment}
Next we investigate how well the reduced basis model we have constructed is able to reproduce the initial conditions on the wire. Simulated data $\bfy_d=\bfy(\bfp_0)$ is produced using a sample parameter value $\bfp_0$ from the uniform distribution over the parameter space $\Pcal$. The problem in the full model case is to find the parameter $\bfp$ that satisfies  
\begin{equation}\label{eq:DiffNorm}
\bfp=\argmin_{\bfp' \in \Pcal} ||\ybf_d-\ybf(\pbf')||^2.
\end{equation}
In the reduced model this becomes finding the reduced parameter $\bfp_r$ that satisfies
\begin{equation}\label{eq:DiffNormRed}
\bfp_r=\argmin_{\bfp'_r \in \Pcal_r} ||\ybf_d-\ybf(\pbf'_r)||^2.
\end{equation}
For each of the full basis dimensions $N=5,10,15,20,25,30,40,50,60,70,80,90,100$, ten simulated data sets were constructed. Equations (\ref{eq:DiffNorm}) and (\ref{eq:DiffNormRed}) were solved using Nelder-Mead minimisation with an initial guess of $\bfp_0+\Delta \bfp_0$ for the full case and $\bfp_0+ Q_k \Delta \bfp_0$ in the reduced case, where the error $\Delta \bfp_0$ is randomly sampled from the uniform distribution over $\Pcal$ and in each case normalised such that $\|\Delta \bfp_0\|=0.05$ and $\|Q_k \Delta \bfp_0\|=0.05$ respectively. 

%\begin{figure}[h]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/AvNitsdim.pdf}
%  \caption{Mean iterations to solve the minimisation problem vs full basis dimensions, with the full basis results in blue and reduced basis results in red. }\label{fig:AvNitsDim}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/AvFcalldim.pdf}
%  \caption{Mean function calls to solve the minimisation problem vs full basis dimensions, with the full basis results in blue and reduced basis results in red.}\label{fig:AvFcalldim}
%\endminipage
%\end{figure}

The number of iterations and function calls required to solve (\ref{eq:DiffNorm}) and (\ref{eq:DiffNormRed}), averaged over the ten simulations, are shown in Figures \ref{fig:AvNitsDim} and \ref{fig:AvFcalldim} respectively. These plots show an exponential increase in iterations and function calls required to solve the optimisation problem in the full basis, and a sub-exponential relationship between problem dimension and the number of iterations and function calls in the reduced basis space. Further work is required to determine the nature of this relationship.

\section{Conclusions and further work}
These results show the potential for improving the computational efficiency in solving inverse problems using reduced basis methods. Further work is required to investigate the sensitivity of this approach to errors in the initial guess in the optimisation algorithm and noise in the data. The greedy method of constructing the reduced basis should be compared to alternative methods such as proper orthogonal decomposition. 


%\bibliography{research}
%\bibliographystyle{abbrv}

%\end{document}                                                                                                      