%\input{ThesisPreamble}
%\begin{document}

\chapter{Model Reduction for Compact Binary Coalescence}

The highly anticipated detection of gravitational waves in 2015 from the coalescence of two stellar mass black holes heralded the beginning of a new era of astronomy. For the foreseeable future, compact binary coalescenses (CBCs) will remain the most detectable sources of gravitational waves. The the detection of these events requires high fidelity waveforms for use in the matched filtering pipeline. However, calculating these waveforms can take weeks to months of supercomputer resources, so adequately surveying the high dimensional parameter space is a computationally infeasible undertaking.

A gravitational waveform is represented in terms of its two polarisations $h_+(t;\lambda)$ and $h_\times(t;\lambda)$ by $h(t;\lambda)=h_+(t;\lambda)+ih_\times(t;\lambda)$, where $\lambda \in \mathcal{P} \subset \mathbb{R}^8$ is a vector describing the binary source parameters (masses, spin magnitudes, spin directions). We denote the space of gravitational waveforms by $X$. An inner product on this space is given by the complex scalar product 
\begin{equation}
<h(\cdot; \lambda_1), h(\cdot; \lambda_2)> = \int^{t_{max}}_{t_{min}} h^*(t; \lambda_1) h(t;\lambda_2) dt
\end{equation}
with an inherited norm given by $\| h(\cdot;\lambda) \|^2 = <h(\cdot; \lambda), h(\cdot; \lambda)>$. We may write $h(\lambda)$ to denote the full waveform, i.e. $h:\Pcal \rightarrow X$, or alternatively $h(t;\lambda)$, in which case $h(t;\lambda)$ denotes the value at time $t$, i.e. $h: \Rbb \times \Pcal \rightarrow \Cbb$. 

\textbf{Question} What is the space $X$? What space do $h_+$,$h_\times$ reside in?

The space $X$ may have very high dimension, however the space of gravitational waveforms resides on a parametrically induced manifold $\Mcal=\{ h(\lambda) : \lambda \in \Pcal\}$, which may be of comparitively low dimension. Thus, for arbitrary $\lambda$ we seek to represent $h(\lambda)$ as a linear combination of some basis for $\mbox{span}\{\Mcal \}\subset X$, that is 
\begin{equation}
h(t;\lambda) \approx \sum_{i=1}^N A_i (\lambda) e_i(t)
\end{equation}
for some basis $e_i$, and as small an $N$ as possible. This raises some immediate questions.
\begin{enumerate}
\item How do we optimally choose the the best $N$-dimensional subspace
\item How do we choose a basis for this subspace
\item How do we best combine this basis to approximate $h(\lambda)$? 
\end{enumerate}

To answer the first question, we set the benchmark by defining the Kolmogorov spaces, $X^{Kol}_N$ as
\begin{equation}
X^{Kol}_N = \mbox{arg }\underset{X_N \subset X}{\mbox{inf}}\left(\underset{\lambda \in \Pcal}{\sup} \underset{h_N \in X_N}{\mbox{ inf}} \| h(\lambda) - h_N \|_X \right)
\end{equation} 
where $X_N$ denotes any $N$-dimensional subspace of $N$. The Kolmogorov $N$-width is then 
\begin{equation}
\epsilon^{Kol}_N = \underset{\lambda \in \Pcal}{\sup} \underset{h_N \in X^{Kol}_N}{\mbox{ inf}} \| h(\lambda) - h_N \|_X .
\end{equation} 
Conceptually, the Kolmogorov spaces are those that minimise the error between $h(\lambda)$ for any $\lambda \in \Pcal$ and the ``closest'' element of that space, and the Kolmogorov $N$-width is the supremum of that error. This is a very strong definition, however actually finding such a space is impractical. 

\section{Empirical Interpolation Method}

We seek to find approximations to elements of $\Mcal$ by developing an operator $\Ical$ to interpolate the waveforms $h(\dot;\lambda)$. The method takes as input a tolerance $\eps$, and optionally a maximal number of iterations $N_{max}$. After $N$ iterations, the output is a set of $N$ basis functions $\{e_i\}_{i=1}^N$ and interpolation points $\{T_i\}_{i=1}^{N}$. Given these, the interpolant $\Ical_N h(\dot;\lambda)$ of $h(\lambda)$ can be expressed as
\begin{equation}
\Ical_N h(t;\lambda) = \sum_{i=1}^N  A_i (\lambda) e_i(t).
\end{equation}
The interpolant satisfies the $N$ interpolation constraints
\begin{equation}
\Ical_N h(T_i;\lambda) = h(T_i;\lambda),
\end{equation}
leading to the following matrix equation
\begin{equation}
\Vbb \Abf(\lambda)=\hbf(\lambda),
\end{equation}
where $(\Vbb)_{ij}=e_j(T_i)$, $(\Abf(\lambda))_i=A_i(\lambda)$, and $(\hbf)_i=h(T_i;\lambda)$. The algorithm proceeds as follows.

\begin{algorithm}
\caption{The Empirical Interpolation Method (Continuous)}\label{alg:EIMc}
\textbf{Input:} $N_{max}$, $\eps$
\begin{algorithmic}[1]
\State $N=0$, $\eps'=\eps+1$,  $\Ical_0 h(t;\lambda)=0$
\State $\lambda_1=\mbox{arg}\underset{\lambda \in \Pcal}{\max}\|h(\cdot;\lambda)\|_{L^\infty(\R)}$
\While{ $N<N_{max}$ and $\eps'>\eps$}
\State $N \gets N+1$
\State $r(t)=h(t; \lambda_N)-\Ical_{N-1}h(t;\lambda_N)$
\State $T_N=\mbox{arg}\underset{t \in \R}{\max} \|r(t) \|$
\State $e_N(t)=r(t)/r(T_N)$
\State $[\eps', \lambda_{N+1}]=\mbox{arg}\underset{\lambda \in \Pcal}{\max}\|h(\cdot;\lambda)-\Ical_{N}h(t;\lambda)\|_{L^\infty(\R)}$
\EndWhile
\end{algorithmic}
\textbf{Output:} $\{e_i\}_{i=1}^N$, $\{T_i\}_{i=1}^{N}$
\end{algorithm}

This produces a sequence of hierarchical spaces $X_1 \subset X_2 \subset \ldots X_N$ where $X_n=\span{e_i}_{i=1}^n$. 

We start with a training set of parameter values $\Xi_{train}=\{\lambda_i\}_{i=1}^M \subset \Pcal$ and associated waveforms $\{h(\lambda_i)\}$. The EIM method proceeds by identifying a set of parameter values $\left\lbrace \Lambda_1, ... \Lambda_N \right\rbrace$ and associated waveforms $ \left\lbrace h_1(t) ... h_N(t) \right\rbrace$ where $h_i(t)=h(t; \Lambda_i)$ to form a basis for the greedy subspace $X_N$. 


The spaces constructed are hierarchical, that is, for $n<n'$, $X_n \subset X_{n'}$. 

An ideal greedy algorithm would 


Explore elementary effects.

\chapter{Reduced basis methods for parameterised PDEs}

%\chapter{Parameterised Models}
%We consider a model that can be described in four distinct steps. First, a parameter value is chosen from the $m$ dimensional parameter space. The initial condition is then found by solving a parameterised elliptic partial differential equation for the chosen parameter $p$. The time evolution is performed by solving a hyperbolic equation with the previously calculated initial condition, and the data is extracted from these solutions. 
%\begin{equation}
%\mathbf{p}\in \mathbb{R}^N \rightarrow u_0 \in H[0,1] \rightarrow u \in L_2 (\mathbb{R}, H) \rightarrow y \in L_2(\mathbb{R})
%\end{equation}
%We are interested in reconstructing the parameters $\bfp$ from (possibly noisy) output data $\bfy_d$. This involves finding the solution to an inverse problem. We pose this as a statistical inverse problem, and apply reduced basis methods for efficiency.

\section{Forward model}
Let $\lambda \in \mathcal{P} \subset \mathbb{R}^N$ be a parameter vector, and $\Omega \in \mathbb{R}^d$ be our function domain. The forward model $\mathcal{M}: \mathcal{P}\times \Omega \rightarrow $

\section{Statistical Inverse Problems}
In a deterministic setting, regularisation and optimisation techniques are used to find a single point estimate of the parameter $\bfp$. A statistical formaulation returns a probability density function over parameter space describing the relative likelihood of observation-consistent parameters, called the `posterior distribution', $\pi(\bfp| \bfy_d)$.

Here we infer properties of the probability distribution of model parameters $\bfp\in\R^N$ 
from observations $\bfy_d\in\R^m$. 
We adopt a Bayesian approach where a prior probability density $\pi(\bfp)$ of $\bfp$ is given. 
We also assume that our model of the observations provides us with
a likelihood function $\pi(\bfy_d\mid \bfp)$ of the data $\bfy_d$. 
The data $\bfy_d$ is treated as a random vector $\bfy$ with probability distribution $\pi(\bfy\mid \bfp)$ and we denote the expectation as
\begin{equation*}
\bfy(\bfp) := E(\bfy\mid \bfp).
\end{equation*}

Bayes' law leads to a formula for the posterior probability density of $\bfp$ as
\begin{equation}\label{eqn:bayes}
\pi(\bfp\mid \bfy_d) \propto \pi(\bfp)\,\pi(\bfy_d\mid \bfp).
\end{equation}
This probability density could then be further explored for example, in order to find marginals or
moments of the posterior or even the MAP (maximum a posteriori) estimate which is of the form
\begin{equation}\label{eqn:PMAPfull}
\bfp_{\MAP} = \argmax_{\bfp\in\R^N} \pi(\bfp\mid \bfy_d).
\end{equation}
In any case, the exploration of the posterior $\pi(\bfp\mid \bfy_d)$ requires its numerical evaluation typically many times. 
In our case each evaluation of the likelihood (and thus the posterior) 
requires an expensive numerical simulation.

\subsection{Reduced basis method}

In order to decrease the computational load required we reduce the parameter space from $\R^N$ to $\R^{N_r}$ as any exploration or computation in a lower dimensional space is cheaper. 
For this we generate a sequence of parameter vectors $\bfp_1, \bfp_2,\ldots$ in $\R^N$ which are defined using a greedy algorithm %as in \citep{Lieberman:2010:PSM:1958688.1958693}
\begin{equation}\label{eq:greedyOpt}
\bfp_{k+1} = \argmax_{\bfp\in\R^N} \left(\frac{1}{2}\|\bfy(\bfp)-\bfy(Q_k \bfp)\|^2 + \beta \log \pi(\bfp)\right),
\end{equation}
where the $Q_k:\R^N\rightarrow \R^N$ are orthogonal projections with 
$\range(Q_k)=\span\{\bfp_1,\ldots,\bfp_k\}$ for $k\geq 1$ and $Q_0:=0$. 
This choice introduces the new $\bfp_{k+1}$ to make sure that a large number of observations $\bfy_d$ can be approximated in the range of $Q_{k+1}$ while exploring parameter vectors which are sufficiently likely according to the prior.


Numerical methods are used to solve PDEs, however for complicated problems the number of parameters required may be quite large, resulting in prohibitive computational cost. The reduced basis method is a means of reducing the number of degrees of freedom in order to reduce costs and make solving the problem feasible. Sometimes we are interested in the field that is the solution to the PDE, perhaps a termperature function. Other times we are intersted in a function of this field, perhaps the maximum temperature. This method evaluates the relationship between the input parameters and the output function, hopefully bypassing many of the expensive computations. This is useful in many real-time and multiple query contexts.

Consider a parameterised model $\Mcal:\Pcal \rightarrow \Ucal \rightarrow \mathcal{Y}$ of some phenomenon, where given a parameter value $\pbf$ in the parameter space $\Pcal\subset\R^N$, the model predicts a state $\bfu(\bfp)$ in the state space $\mathcal{U}\subset\R^K$ from which we can extract output  $\ybf(\pbf)\in \mathcal{Y}\subset\R^M$. Suppose in an experimental setting we obtain noisy observations $\ybf_d\in \mathcal{Y}$. The inverse problem is to estimate $\pbf$ given $\ybf_d$, a problem that is often ill-posed. 

In order to decrease the computational load required we reduce the parameter space from $\R^N$ to $\R^{N_r}$ as any exploration or computation in a lower dimensional space is cheaper. 
For this we generate a sequence of parameter vectors $\pbf_1, \pbf_2,\ldots$ in $\R^N$ which are defined using a greedy algorithm 
\begin{equation}\label{eq:greedyOpt}
\pbf_{k+1} = \argmax_{\pbf\in\R^N} \left(\frac{1}{2}\|\ybf(\pbf)-\ybf(Q_k \pbf)\|^2 + \beta \log \pi(\pbf)\right),
\end{equation}
where the $Q_k:\R^N\rightarrow \R^N$ are orthogonal projections with 
$\range(Q_k)=\span\{\pbf_1,\ldots,\pbf_k\}$ for $k\geq 1$ and $Q_0:=0$. 
This choice introduces the new $\pbf_{k+1}$ to make sure that a large number of observations $\ybf_d$ can be approximated in the range of $Q_{k+1}$ while exploring parameter vectors which are sufficiently likely according to the prior. This method is discussed in detail in \cite{lieberman2009}parameter.
%
%In this paper we will demonstrate the Reduced Basis Method for solving a problem of the form 
%\begin{equation}
%\Mcal:\Pcal \rightarrow \Ucal_0 \rightarrow \Ucal_1 \rightarrow \mathcal{Y},
%\end{equation}


\section{One dimensional wire}
Consider a simple one dimensional case. We have a wire fixed at ends $x=0$ and $x=1$, under constant tension $T=1$, and subject to a transverse load $f(x;p)$, where $p \in \mathcal{P}$ is our parameter. Let $u(x,t;p)$ denote the transverse displacement of the wire at point $x$ and time $t$, with boundary condition $u(0,t;p)=u(1,t;p)=0$. We find the initial condition $u_0(x;p)=u(x,0;p)\in H_0[0,1]$ by solving the one dimensional Poisson equation
\begin{equation}\label{eq:1dConstraint}
-\frac{\pd^2 u_0}{\pd x^2}(x) = f(x;p). 
\end{equation}
The system time evolution is governed by the one dimensional wave equation, where the wire is assumed to be released from rest
\begin{equation}\label{eq:1dEvolution}
\frac{\pd^2 u}{\pd t^2}=c^2 \frac{\pd^2 u}{\pd x^2}
\end{equation}
The observations $\mathbf{y}_i \in \Rbb^j$ are a set of time series vectors of the displacement amplitudes at some points $x_i \in [0,1]$ at $j$ sample times. 

\subsection{Exact Solution}
In this example we can explicitly compute the operator $\Mcal$. The solution to (\ref{eq:1dConstraint}) is 
\begin{equation}\label{eq:1DInitCond}
u_0(x;p)=\int_0^1 G(x,\xi) f(\xi;p) d \xi,
\end{equation}
Where $G(x,\xi)$ is Green's function given by
\begin{equation}
\label{eq:Greens}
G(x,\xi)=\begin{cases} \xi(1-x) \mbox{ if } 0\leq \xi \leq x \\
					  x(1-\xi) \mbox{ if } x \leq \xi \leq 1.
		\end{cases}
\end{equation}
The general solution to (\ref{eq:1dEvolution}) is
\begin{equation}\label{eq:1Ddisplacements}
u(x,t;p)=\sum^{\infty}_{n=1} \beta_n(p)\sin(n\pi x)\cos(c n \pi t). 
\end{equation}
We find by substituting $t=0$ that the $\beta_n$ are the Fourier sine series coefficients of $u_0$:
\begin{equation}
\beta_n(p)=2\int_0^1 u_0(x;p) \sin(n\pi x) dx.
\end{equation}
The output $\mathbf{y}$ is found by substituting the observation points $\bfx$ into (\ref{eq:1DInitCond}):
\begin{equation}\label{eq:1Doutput}
\bfy(t)=\sum^{\infty}_{n=1} \beta_n(p)\cos(c n \pi t)\sin(\bfx n\pi).
\end{equation}


\subsection{Numerical Solution}
We now develop the operator $\Mcal$ for the numerical solution to the system.

\subsubsection{Initial condition}
Consider a finite differencing scheme. Our domain $[0,1]$ is divided into $n$ subintervals of length $h=\Delta x = 1 /n$ so that the endpoints of the intervals are given by $x_j = j  \Delta x = jh$ for $j= 0, 1, \cdots n$. Using a Taylor expansion of $u$ around $x$ we find
\begin{equation}
\begin{array}{lcl}
-u_{xx}(x)=\frac{-u(x+h)+2u(x)-u(x-h)}{h^2}&=& f(x)  \mbox{ by eq \ref{eq:1dConstraint}} \\
\rightarrow \frac{-u_{j+1} + 2u_j - u_{j-1} }{h^2} = f(x_j)
\end{array}
\end{equation}
where $u_i = u(x_i)$. By setting the endpoints according to our boundary conditions $u_0 = u_n=0$ and defining
\begin{equation}
\hat{u}=[u_1,...,u_{n-1}]^T \mbox{ , } \hat{f} = [f(x_1),...,f(x_{n-1})], A= \left[\begin{array}{ccccc}
2  & -1 & 0  & \cdots & 0\\
-1 & 2  & -1 &  & \vdots \\
0  & -1 & 2  & \ddots & 0 \\
\vdots & & \ddots & \ddots & -1 \\
0 & \cdots & 0 & -1 & 2 
\end{array}\right]
\end{equation}
the initial value problem then becomes finding the solution to the matrix eq
\begin{equation}
A \hat{u} = h^2 \hat{f}
\end{equation}
the solution of which is 
\begin{equation}
\hat{u}_0(p) = A^{-1}h^2\hat{f}(p)
\end{equation}

\subsubsection{Evolution}
Defining $\hat{u}(x,t)$ similarly to above, \ref{eq:1dEvolution} becomes
\begin{equation}
\frac{d^2 \hat{u}}{dt^2}+c^2 A \hat{u}=0
\end{equation}
with initial conditions $\hat{u}(0,p)=\hat{u}_0(p)$ and $\hat{u}'(0,p)=\hat{u}_1(p)$. This admits the solution
\begin{equation}\label{eq:dispSoln}
\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)A^{-1}\hat{f}(p)
\end{equation}

%\subsection{results for different fs}
%\subsubsection{$f(x)= \sin(p \pi x)$}
%$f(x)= \sin(p \pi x)$ where $p \in \mathcal{P} \subset \mathbb{R}_+$ is the spatial frequency of the load. We take the wave speed $c=1$. 
%The initial condition $u_0(x)=\frac{1}{\pi^2 p^2}\sin(\pi p x)$, so we see that the spatial frequency and the amplitude of the initial condition depend on the parameter $p$. Likewise, the full solution $u(x,t)=\frac{1}{\pi^2 p^2}\sin(\pi p x)\cos(\pi p c t)$. The observations $\mathbf{y}$ are given by
%
%\begin{equation}
%\mathbf{y}=\left[
%\begin{array}{c}
%u(0.1,t)\\
%u(0.2,t)\\
%u(0.4,t)\\
%u(0.8,t)\\
%\end{array}
%\right]
%= \frac{\cos(\pi p c t)}{\pi^2 p^2} \left[
%\begin{array}{c}
%\sin(0.1 \pi p) \\
%\sin(0.2 \pi p) \\
%\sin(0.4 \pi p) \\
%\sin(0.8 \pi p) \\
%\end{array}
%\right]
%\end{equation}

%\subsection{Experiment 1}
%To start, let us consider just the evolution equation \ref{eq:1dEvolution}. Take the parameter to be simply the initial displacements at the internal gridpoints $\hat{\bfx}=[h,2h,\cdots, Nh]^T$, $h=1/(N+1)$. 
%\begin{equation}
%\label{eq:InitCondNew}
%\bfp = \hat{u_0}(\hat{\bfx})
%\end{equation}
%This means that (\ref{eq:dispSoln}) becomes
%\begin{equation}
%\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)\bfp
%\end{equation}
%Let us take the output to be the displacement over times $t_1,\cdots, t_m$ just at one point $x_s$. 
%\begin{equation}
%\bfy=\left[
%\begin{array}{c}
%\bfe_s^T \cos(cA^{\frac{1}{2}}t_1) \\
%\vdots\\
%\bfe_s^T \cos(cA^{\frac{1}{2}}t_m)
%\end{array}
%\right]\bfp \equiv M \bfp
%\end{equation}
%where $\bfe_s=[0,\dots,1\dots,0]$ with the $1$ in the $s^{th}$ position. We investigate how well we are able to approximate the output $\bfy(\bfp)$ by reduced models $\bfy(Q_k \bfp)$ using $N=[5,10,20,40]$ when the initial displacement is given by
%\begin{equation}
%u_0(x) = \sum_{k=1}^5 c_k \sin(2\pi k x)
%\end{equation}
%where the $c_k$ are uniformly distributed in $[0,1]$. The parameter space $\mathcal{P}={\bfp \in \mathbb{R}^N | \| \bfp \| = 1}$. 
%
%We define approximations to the parameter vectors $\bfp_k$ defined in equation~(\ref{eq:greedyOpt}) by
%\begin{equation}
%\bfp_{k+1} = \argmax_{\bfp\in \Mcal_k }\frac{1}{2}\|\bfy(\bfp)-\bfy(Q_k \bfp)\|^2 .
%\end{equation}
%where each $\Mcal_k$ contains $m$ independent samples of the uniform distribution over $\Pcal$ and the the elements of $\Mcal_k$ and $\Mcal_j$ are independent for $j\neq k$. In the experiment we chose $m=100$. Note that this allows us to drop the $\beta \log \pi(\bfp)$ term that appears in equation (\ref{eq:greedyOpt}). We denote the approximations of $\bfp_k$ by the same symbol $\bfp_k$ for simplicity.
%
%To quantify how well the reduced model approximation $\ybf(Q_k\pbf)$ can recover the full model $\ybf(\pbf)$ for arbitrary $\pbf$, we use the relative $L_2$ norm 
%$$\frac{\sqrt{\int_\Pcal ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2\, d\omega}}{\sqrt{\int_\Pcal ||\ybf(\pbf)||^2\, d\omega}},$$ 
%where $\omega$ is a probability measure over $\Pcal$.
%We approximate this relative root mean squared error by Monte Carlo quadrature as
%\begin{equation}\label{eq:MCrmse}
%\rho_k = \frac{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2}}{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)||^2}}.
%\end{equation}
%
%\begin{figure}[t]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/Exp1results/rmsevsndim.pdf}
%  \caption{Relative root mean square error calculted using equation \ref{eq:MCrmse} }\label{fig:Exp1rmsevecs}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/Exp1results/RBvecsreqforeps.pdf}
%  \caption{Number of basis vectors required to achieve relative error less than $\epsilon=1\times 10^{-10}$}
%\endminipage
%
%\end{figure}
%
%Figure \ref{fig:Exp1rmsevecs} shows $\rho_k$ vs $k$ for $N=5,10,20,40$. For $N=5$ only two basis vectors are required to achieve accuracy within $\epsilon=1\times 10^{-10}$. This is due to the symmetry of the function about $x_2=0.5$, with $u(x_0)=-u(x_4)$ and $u(x_1)=-u(x_3)$ leaving just two independent components. Four basis vectors are required for $N=10,20,40$ which reflects the four basis functions used to construct $u_0$.

\subsection{Reduced basis construction}
To start, let us consider just the evolution equation (\ref{eq:1dEvolution}). We construct the initial condition as follows
\begin{equation}
\label{eq:Exp2initcond}
u_0 = \sum_{n=1}^N p_n \sin(2\pi n x)/n^2,
\end{equation}
where the parameter $\bfp=[p_n]$ and (\ref{eq:dispSoln}) becomes
\begin{equation}
\hat{u}(t;p)=\cos(c A^{\frac{1}{2}}t)\left[ \begin{array}{ccc}
\sin(2\pi \bfx) & \cdots & \sin(2N \pi \bfx)/N^2 
\end{array}
\right]\bfp.
\end{equation}

The number of gridpoints is fixed at $n=100$. Let us take the output to be the displacement over times $t_1,\cdots, t_m$ just at one gridpoint $x_s$. 
\begin{equation}
\bfy=\left[
\begin{array}{ccc}
\bfe_s^T \cos(cA^{\frac{1}{2}}t_1)\sin(2\pi \bfx) & \cdots & \bfe_s^T \cos(cA^{\frac{1}{2}}t_1)\sin(2N \pi \bfx)/N^2 \\
\vdots & \ddots & \vdots \\
\bfe_s^T \cos(cA^{\frac{1}{2}}t_m)\sin(2\pi \bfx) & \cdots & \bfe_s^T \cos(cA^{\frac{1}{2}}t_m)\sin(2N \pi \bfx)/N^2
\end{array}
\right]\bfp \equiv M \bfp
\end{equation}
where $\bfe_s=[0,\dots,1\dots,0]$ with the $1$ in the $s^{th}$ position. 

We investigate how well we are able to approximate the output $\bfy(\bfp)$ by reduced models $\bfy(Q_k \bfp)$. %
%
%The system matrix is $A=MB$ where $M$ is as above and 
%\begin{equation}
%B=\left[ \begin{array}{ccc}
%\sin(2\pi \bfx) & \cdots & \sin(2N \pi \bfx)/N^2 
%\end{array}
%\right]
%\end{equation}
We define approximations to the parameter vectors $\bfp_k$ defined in equation (\ref{eq:greedyOpt}) by
\begin{equation}
\bfp_{k+1} = \argmax_{\bfp\in \Mcal_k }\frac{1}{2}\|\bfy(\bfp)-\bfy(Q_k \bfp)\|^2 .
\end{equation}
where each $\Mcal_k$ contains $m$ independent samples of the uniform distribution over $\Pcal$ and the the elements of $\Mcal_k$ and $\Mcal_j$ are independent for $j\neq k$.  Note that this allows us to drop the $\beta \log \pi(\bfp)$ term that appears in equation (\ref{eq:greedyOpt}). We denote the approximations of $\bfp_k$ by the same symbol $\bfp_k$ for simplicity. Here $ |\Mcal_k| = 100$, and $Q_k = P_k (P_k^T P_k)^{-1} P_k^T$ where 
\[ P_k = [ \bfp_1 \cdots \bfp_k ]. \] 

To quantify how well the reduced model approximation $\ybf(Q_k\pbf)$ can recover the full model $\ybf(\pbf)$ for arbitrary $\pbf$, we use the relative $L_2$ norm 
$$\frac{\sqrt{\int_\Pcal ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2\, d\omega}}{\sqrt{\int_\Pcal ||\ybf(\pbf)||^2\, d\omega}},$$ 
where $\omega$ is a probability measure over $\Pcal$.
We approximate this relative root mean squared error by Monte Carlo quadrature as
\begin{equation}\label{eq:MCrmse}
\rho_k = \frac{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)-\ybf(Q_k\pbf)||^2}}{\sqrt{\frac{1}{m} \sum_{\pbf\in \Mcal_{k+1}} ||\ybf(\pbf)||^2}}.
\end{equation}

Reduced bases were constructed for full basis dimensions $N=5, 10, 15, 20,$ $25, 30, 40,$ $50,$ $60,$ $70,$ $80, 90, 100$. Figure \ref{fig:Exp2rmsevecs} shows $\rho_k$ vs $k$ for each $N$, calculated using (\ref{eq:MCrmse}), and figure \ref{fig:Exp2vecsreq} shows the number of reduced basis vectors required to achieve $\rho_k < 0.05$. This shows that for more complex problems (a higher $N$), more reduced basis functions are required to achieve the error threshold, but the relative number of basis functions required decreases. In the case of $N=100$, a reduced model error of $\rho_k<0.05$ is achieved with $48$ basis vectors, a reduction in the parameter space dimension of more than half.

%\begin{figure}[h]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/rmsevsndim.pdf}
%  \caption{Relative root mean square error calculted using equation (\ref{eq:MCrmse}) }\label{fig:Exp2rmsevecs}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/RBvecsreqforeps.pdf}
%  \caption{Number of basis vectors required to achieve relative error less than $\rho_k=0.05$}\label{fig:Exp2vecsreq}
%\endminipage
%\end{figure}

\subsubsection{Inverse Experiment}
Next we investigate how well the reduced basis model we have constructed is able to reproduce the initial conditions on the wire. Simulated data $\bfy_d=\bfy(\bfp_0)$ is produced using a sample parameter value $\bfp_0$ from the uniform distribution over the parameter space $\Pcal$. The problem in the full model case is to find the parameter $\bfp$ that satisfies  
\begin{equation}\label{eq:DiffNorm}
\bfp=\argmin_{\bfp' \in \Pcal} ||\ybf_d-\ybf(\pbf')||^2.
\end{equation}
In the reduced model this becomes finding the reduced parameter $\bfp_r$ that satisfies
\begin{equation}\label{eq:DiffNormRed}
\bfp_r=\argmin_{\bfp'_r \in \Pcal_r} ||\ybf_d-\ybf(\pbf'_r)||^2.
\end{equation}
For each of the full basis dimensions $N=5,10,15,20,25,30,40,50,60,70,80,90,100$, ten simulated data sets were constructed. Equations (\ref{eq:DiffNorm}) and (\ref{eq:DiffNormRed}) were solved using Nelder-Mead minimisation with an initial guess of $\bfp_0+\Delta \bfp_0$ for the full case and $\bfp_0+ Q_k \Delta \bfp_0$ in the reduced case, where the error $\Delta \bfp_0$ is randomly sampled from the uniform distribution over $\Pcal$ and in each case normalised such that $\|\Delta \bfp_0\|=0.05$ and $\|Q_k \Delta \bfp_0\|=0.05$ respectively. 

%\begin{figure}[h]
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/AvNitsdim.pdf}
%  \caption{Mean iterations to solve the minimisation problem vs full basis dimensions, with the full basis results in blue and reduced basis results in red. }\label{fig:AvNitsDim}
%\endminipage\hfill
%\minipage{0.48\textwidth}
%  \includegraphics[width=\linewidth]{Chenni/AvFcalldim.pdf}
%  \caption{Mean function calls to solve the minimisation problem vs full basis dimensions, with the full basis results in blue and reduced basis results in red.}\label{fig:AvFcalldim}
%\endminipage
%\end{figure}

The number of iterations and function calls required to solve (\ref{eq:DiffNorm}) and (\ref{eq:DiffNormRed}), averaged over the ten simulations, are shown in Figures \ref{fig:AvNitsDim} and \ref{fig:AvFcalldim} respectively. These plots show an exponential increase in iterations and function calls required to solve the optimisation problem in the full basis, and a sub-exponential relationship between problem dimension and the number of iterations and function calls in the reduced basis space. Further work is required to determine the nature of this relationship.

\section{Conclusions and further work}
These results show the potential for improving the computational efficiency in solving inverse problems using reduced basis methods. Further work is required to investigate the sensitivity of this approach to errors in the initial guess in the optimisation algorithm and noise in the data. The greedy method of constructing the reduced basis should be compared to alternative methods such as proper orthogonal decomposition. 


%\bibliography{research}
%\bibliographystyle{abbrv}

%\end{document}                                                                                                      