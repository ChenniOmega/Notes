%\input{ThesisPreamble}
%\begin{document}

\chapter{Data analysis}           
Our data model is of a continuous time stochastic process $X(t)$ over an interval $[0,T]$, sampled at $n$ points ${t_1, \ldots, t_n} \subset T$. We assume an additive noise model, that is 
\begin{equation}
X_t=h_t+N_t, \hspace{12pt} t\in {t_1, \ldots, t_n}  ,
\end{equation}
where $X_t=X(t)$, $h_t$ is a sample from a deterministic signal waveform at time $t$, and $\{N_t: t \in [0,T]\}$ is the detector noise, a zero mean Gaussian random process with covariance matrix $\Sigma_N$. 


\section{Hypothesis Testing}
Gravitational wave detection can be cast as a hypothesis testing problem. The \textit{null hypothesis}, $H_0$, is that the signal is absent and the data consists entirely of noise with probability distribution $P_0$. The \textit{alternative hypothesis}, $H_1$, is that a signal is present and the data has probability distribution $P_1$. This can be written
\begin{equation}
\label{eq:hyppair}
\begin{array}{ll}
H_0: X_t = N_t \sim P_0, &\hspace{12pt} t\in {t_1, \ldots, t_n} \\
H_1: X_t = h_t + N_t \sim P_1, &\hspace{12pt} t\in {t_1, \ldots, t_n}. 
\end{array}
\end{equation}
A \textit{hypothesis test} or \textit{decision rule} $\delta$ is a partition of the observation set $\Xcal$ into two subsets $\Xcal_0$ and $\Xcal_1=\Xcal_0^c$. If the data  $x \in \Xcal_0$, we accept the null hypothesis $H_0$. If $x \in \Xcal_1$, we claim a detection!

We now need a way to choose a decision rule $\delta$ that is optimal in some sense. There are several several ways to do this; here the Neyman-Pearson approach is described. The Bayesian and Minimax approaches are also common, and can be found in [poor].

\subsection{Neyman-Pearson Approach}
There are two types of errors that can occur in testing $H_0$ against $H_1$. A \textit{type I error} or \textit{false alarm} occurs if we choose $H_1$ when $H_0$ is true, and the probability of this occuring for a decision rule $\delta$ is called the \textit{false alarm probability} denoted $P_F(\delta)$. The second error occurs if we choose $H_0$ when $H_1$ is true and is called a \textit{type II error} or \textit{miss}. We can define a \textit{miss probability}, $P_M(\delta)$, however it is more common to talk in terms of the \textit{detection probability} or \textit{power} of the test $P_D(\delta)=1-P_M(\delta)$. The Neyman-Pearson optimum test maximises the power of the test, subject to a chosen constraint on the false alarm probability:
\begin{equation}
\delta_{NP}=\argmax_{\delta} P_D(\delta) \mbox{ subject to } P_F(\delta)\leq \alpha.
\end{equation}
The bound $\alpha$ is called the \textit{significance level} of the test. We seek to devise a function $\phi: \Xcal \rightarrow [0,1]$ representing the probability of choosing $H_1$ given some data $x\in \Xcal$. The detection and false alarm probabilities for a decision rule associated with such a function are as follows:
\begin{eqnarray}
P_D(\delta) = E_1[\phi(\Xcal)]  &=& \int_{\Xcal} \phi(x) p_1(x) d\bfx  \label{eq:PDrandomised} \\
P_F(\delta) = E_0[\phi(\Xcal)]  &=& \int_{\Xcal} \phi(x) p_0(x) d\bfx \label{eq:PFrandomised}
\end{eqnarray}
The existence and uniqueness of such a test function is given by the Neyman-Pearson Lemma.

\begin{lemma}\
Consider the hypothesis pair \ref{eq:hyppair}. Suppose $P_0$ and $P_1$ have densities $p_0$ and $p_1$ respectively and $\alpha>0$. Then
\begin{enumerate}
\item \textit{Existence:} For every $\alpha \in (0,1)$, there is a function  $\phi(x)$ associated to a test $\delta_{NP}$ which is of the form 
\begin{equation}
\label{eq:nptest}
\phi(x)=\left\{ 
\begin{array}{ll}
1 & \mbox{ if } p_1(x) > \lambda p_0(x) \\
\gamma & \mbox{ if } p_1(x) = \lambda p_0(x) \\
0 & \mbox{ if } p_1(x) < \lambda p_0(x) \\
\end{array}
\right.
\end{equation}
where $\eta\geq 0$, $0\leq \gamma \leq 1$ and such that $P_F(\delta_{NP})=\alpha$.
\item \textit{Optimality:} Let $\delta'$ be any decision rule satisfying $P_F(\delta')\leq \alpha$, and that it is associated with some function $\phi'$. Then $P_D(\delta_{NP}) \geq P_D(\delta')$, where $\delta_{NP}$ is as defined by the function \ref{eq:nptest}.
\item \textit{Uniqueness:} Suppose that $\delta_{NP}''$ is any $\alpha$-level Neyman-Pearson decision rule for $H_0$ versus $H_1$. Then the function $\phi''(x)$ associated with $\delta_{NP}''$ must be of the form \ref{eq:nptest}, except for possibly on a subset of $\Xcal$ having zero probability under $H_0$ and $H_1$.
\end{enumerate}

\end{lemma}

\begin{proof}

\begin{enumerate}
\item Let $\eta$ be the smallest number such that 
\begin{equation}
P_0(p_1(X)>\eta p_0(X)) \leq \alpha.
\end{equation}
If $P_0(p_1(X)>\eta p_0(X)) < \alpha$, choose 
\begin{equation}
\gamma=\frac{\alpha-P_0(p_1(X)>\eta p_0(X))}{P_0(p_1(X)=\eta p_0(X))},
\end{equation}
otherwise $\gamma$ can be chosen arbitrarily. Note that for a test of the form \ref{eq:nptest}, the false alarm probability is the expectation of the function $\phi$ under $H_0$:
\begin{equation}
P_F(\delta_{NP})=E_0[\phi]=1\times P_0(p_1(X)>\eta p_0(X))+\gamma P_0(p_1(X)=\eta p_0(X)) =\alpha.
\end{equation}
\item Note that $\forall x \in \Xcal$
\[ [\phi(x) - \phi'(x)][p_1(x)-\lambda p_0(x)]\geq 0, \]
and therefore,
\begin{equation}
\int_{\Xcal} [\phi(x) - \phi'(x)][p_1(x)-\lambda p_0(x)] d\xbf \geq 0.
\end{equation}
Expanding this and rearranging terms gives 
\begin{equation}
\int_{\Xcal}\phi(x) p_1(x) d\xbf - \int_{\Xcal}\phi'(x) p_1(x) d\xbf \geq \lambda \left[ \int_{\Xcal} \phi(x)p_0(x) d\xbf - \int_{\Xcal}\phi'(x)p_0(x) d\xbf \right],
\end{equation}
which, using \ref{eq:PDrandomised} and \ref{eq:PFrandomised}, becomes
\begin{equation}\label{eq:NPL2}
P_D(\delta_{NP}) - P_D(\delta') \geq \lambda \left[\underbrace{P_F(\delta_{NP})}_{=\alpha} - \underbrace{P_F(\delta')}_{\leq \alpha} \right]\geq 0.
\end{equation}
Thus, $P_D(\delta_{NP}) \geq P_D(\delta')$. 
\item By definition, we must have $P_D(\delta_{NP}'')=P_D(\delta_{NP})$. Therefore, by \ref{eq:NPL2} we have
\begin{equation}
0 \geq \lambda \left[ \alpha - P_F(\delta_{NP}'') \right]\geq 0
\end{equation}
or, $P_F(\delta_{NP}'')=\alpha$. Reversing the steps in part 2 above, we find
\begin{equation}
\int_{\Xcal} [\phi(x) - \phi''(x)][p_1(x)-\lambda p_0(x)] d\xbf = 0.
\end{equation}
Since we know the integrand to be non-negative, this implies that it is zero except possibly on a set of zero probability under $H_0$ and $H_1$. Therefore, $\phi(x)$ and $\phi''(x)$ differ only on the set $\{x \in \Xcal | p_1(x)-\lambda p_0(x)=0 \}$, which implies $\phi''$ is also of the form \ref{eq:nptest}, differing only in $\gamma$.
\end{enumerate}
\end{proof}

We define the quantity 
\begin{equation}
L(x)=\frac{p_1(x)}{p_0(x)}, \hspace{12pt} x\in\Xcal,
\end{equation}
to be the \textit{likelihood ratio}. Then, by rearranging the conditions in \ref{eq:nptest}, we see that a Neyman-Pearson test involves comparing the likelihood ratio for an observed value of $X$ to a threshold $\lambda$. 

\section{Introduction}     
For a known waveform the most efficient data analysis tool is matched filtering. But often the waveforms of GW are poorly known, limiting the matched filter's applicability. 

A linear time frequency transform correlates the signal with a family of waveforms that are well concentrated in time and space. These waveforms are called \textit{time frequency atoms}.

\section{Fourier Transforms}
\[\hat{f}(\omega)=\int^{+\infty}_{-\infty}f(t)e{-i\omega t}dt \]

\section{Windowed Fourier Transforms}
Gabor atoms 
\[ g_{u,\xi} (t)=g(t-u)e^{i\xi t} \]
When $g$ is Gaussian, best localisation and the atoms are called \textit{Gabor functions.} Fourier transform of Gabor atom
\[ \hat{g}_{u,\xi} (\omega) =\hat{g}(\omega-\xi) e^{-iu(\omega-\xi)} \]
Windowed Fourier transform:
\[ Sf(u,\xi)=\int^{+\infty}_{-\infty}f(t)g*_{u,\xi}(t)dt=\int^{+\infty}_{-\infty}f(t)g(t-u)e^{-i\xi t}dt \]
Also can be written as frequency integral
\[ Sf(u,\xi)=\frac{1}{2\pi}\int^{+\infty}_{-\infty}\hat{f}(\omega)\hat{g}^{*}_{u,\xi}(\omega)d\omega \]

\section{wavelet transforms}
Wavelet transforms are described in terms of families of basis functions $\psi_{a,b}(x)$ which are dilations and translations of a real function $\psi(x)$ of zero average:
\[ \psi_{a,b}(t)=\frac{1}{\sqrt{|a|}}\psi\left(\frac{t-b}{a}\right) \]
A wavelet transform of a function $f$ has a different time frequency resolution to a windowed Fourier transfrom. It is computed by correlating $f$ with a wavelet atom 
\[Wf(a,b)=\int^{+\infty}_{-\infty}f(t)\psi^{*}_{a,b}(t)dt=\frac{1}{2\pi}\int^{+\infty}_{-\infty} \hat{f}(\omega)\hat{\psi}^{*}_{a,b}(\omega) d\omega \]

\section{Notes from "method for detection and reconstruction of gravitational wave transients with networks of advanced detectors}
\subsection{Low latency search}
Designed to identify potentially significant candidates in real time (within minutes). Streaming? Promptly share sky location with partner telescopes to search for EM counterparts eg Gamma ray burst and afterglow or kilonova which will fade within seconds to days. 

\subsection{Archive analysis}
Establish significance of event and identify progenitors. Slower and more detailed. 

\subsection{Inverse problem for data from a network of detectors}
Network of $K$ detectors. Time series data from the detectors is transformed with the Wilson-Debauchies-Meyer (WDM) transform so that each detector $k$ presents data $x_k[i]$ in a general time-frequency (TF) domain, where $i$ is the data sampling (TF pixel) index. 

Detector noise assumed to be Gaussian, described by WDM power spectral density $S_k[i]$ estimated for every data sample. The noise-scaled (whitened) data is defined as $\omega_k[i] = x_k[i]/\sqrt{S_k[i]}$

TF series combined to obtain energy TF maps $E[i]=\sum_k \omega^2_k [i]$ where the $E[i]$ are maximised over all possible time-of-flight delays in the network (i think this means it's lining up the signals based on the travel time of GW from one detector to another. Obviously this depends on where in the sky the GW is coming from). The energy maps are are used to identify TF areas with excess energy above baseline detector noise using an appropriate clustering algorithm. The inverse problem is to extract the signal waveform, polarisation and sky location. (Note that this slightly differs from my definition of the inverse problem which is to determine the parameters from the waveform. Many steps to the inverse problem). 

Data vector $\bfx [i]={x_1[i], \ldots , x_K[i]}$ recorded by detector at time of a GW signal $\bfh [i] = [h_+[i], h_{\times}[i] ]$ coming from sky location $\theta$ and $\phi$ is a superposition of network response $\Fcal \bfh[i]$ and noise $\bfn[\bfi]$:
\[ \bfx [i] = \Fcal \bfh[i]+\bfn[\bfi] \]
where $\Fcal$ is the network antenna pattern matrix:
\[ \Fcal = \left[ \begin{array}{cc} 
				F_{1+}(\theta, \phi) & F_{1\times}(\theta, \phi) \\
				\ldots				& \ldots	\\
				F_{K+}(\theta, \phi) & F_{K \times}(\theta, \phi) \end{array} \right] \]

(Antenna patterns often also include a transformation by polarisation angle $\Psi$, but this is equivalent to a rotation of the wave frame and can be included in the definition of $\bfh$).

BX-LBTN-KYW2-9V7B-9K2D
\subsection{Matched Filtering}

from \cite{PhysRevD.82.044025}
Searches for GWs from CBC events employ matched filtering as the first step. 

"whitening": Optimal strategy, weight detector output and template wavefor by inverse of amplitude spectral density of detector noise 

discretely sampled time series
whitened data: $\overrightarrow{s}={s_i}$
whitened template waveform: $\overrightarrow{h}_\alpha = h_{\alpha i}$
noise: $\overrightarrow{n}={n_i}$ zero-mean $<n_i>=0$, unit-variance Gaussian random process $<n_i,n_j>=\delta_{ij}$


output of matched filter is given by the vector inner product 
\begin{equation}
\rho_\alpha = \overrightarrow{h}_\alpha^*\cdot \overrightarrow{s}
\end{equation}

search over phase by using complex-valued templates where $\mathfrak{R}\overrightarrow{h}_\alpha$ contains the cosine like phase and $\mathfrak{I}\overrightarrow{h}_\alpha$ contains the sine-like phase. 

\section{Jaranowski et.al}

\subsubsection{Random Variables}
A \textit{probability space} is a triple $(\Xcal, \Ocal, P)$ where
\begin{itemize}
\item $\Xcal$ is a sample space, usually an $n$-dimensional Euclidean space $\Rbb^n$ or a discrete set $\Gamma=\{\gamma_1,\gamma_2, \ldots\}$, or for continuous process, a function space.
\item $\Ocal$ is a $\sigma$-algebra of subsets of $\Xcal$ called observation events, these are the set of values that data can take. 
\begin{itemize}
\item if $\Xcal=\Rbb^n$, consider open intervals 
\begin{equation}
\label{eq:openInts}
\{ \bfy = (y_1,\ldots, y_n)\in \Rbb^n : a_1 < y_1 < b_1, \ldots a_n < y_n < b_n \}
\end{equation}
where $a_i$ and $b_i$ are real numbers. Then usually $\Ocal$ is the smalles $\sigma$-algebra containing all such intervals. This is known as the \textit{Borel} $\sigma$-algebra and is denoted by $B^n$.
\item if $\Xcal$ is a discrete set $\Gamma$, the $\sigma$-algebra $\Ocal$ is the set of all subsets of $\Gamma$. This is called the \textit{power set} of $\Gamma$ and is denoted by $2^{\Gamma}$.
\end{itemize}
\item $P$ is a probability distribution (measure) on $\Ocal$. This is a function assigning a number $P(A)\geq 0$ to any set $A\in \Ocal$ such that $P(\emptyset)=0$ and $P(\Xcal)=1$. For probability spaces $(\Rbb^n, B^n, P)$ usually there exists a \textit{probability density function} $p: \Rbb^n \rightarrow [0,\infty) $ such that 
\begin{equation}
P(A)=\int_A p(\xbf) \d \xbf
\end{equation}
where $\xbf$ are some coordinates on $\Rbb^n$. 
\end{itemize}

$\sigma$-algebra:
\begin{enumerate}
\item $\Xcal \in \Ocal$
\item if $A \in \Ocal$, then $A^c := \Xcal - A \in \Ocal$
\item for any countable family $A_k$ of members of $Ocal$, their union is in $\Ocal$
\end{enumerate}
Together these imply that the intersection of a finite or countable family of members of $\Ocal$ belong to $\Ocal$.

A \textit{random variable} $X$ is a real-valued function $X:\Xcal \rightarrow \Rbb$. The sets $\{ \xi \in \Xcal : X(\xi) \leq x \} \in \Ocal \forall x \in \Rbb$.

The \textit{cumulative distribution function} of $X$ is 
\begin{equation}
\label{eq:cdf}
P_X(x) := P(X\leq x).
\end{equation} 
It is non-decreasing and right-continuous.

A \textit{continuous} random function is one with a continuous cumulative distribution function. If it is differentiable, then the probability density  function is 
\begin{equation}
\label{eq:pdf}
p_X(x)=\frac{\d P_X(x)}{\d x}.
\end{equation}
This is always non-negative as the cdf is non-decreasing. It is also normalised 
\begin{equation}
\label{eq:pdfNormalised}
\int_{-\infty}^{\infty} p_X(x) \d x =1
\end{equation}

The \textit{expectation value} or \textit{mean} of a continuous random variable is 
\begin{equation}
\label{eq:expectation}
E\{X\} := \int {-\infty}^{\infty} x p_X(x) \d x
\end{equation}

The \textit{variance} of the random variable $X$ is 
\begin{equation}
\mbox{Var} \{ X \} \equiv \sigma^2_X := E\{(X-E\{X\})^2\}.
\end{equation}

The \textit{covariance} between two random variables $X$ and $Y$ defined on the same probability space is 
\begin{equation}
\mbox{Cov}\{XY\} := E\{(X-E\{X\})(Y-E\{Y\})\} = E\{XY\}-E\{X\}E\{Y\}.
\end{equation}
Random variables for which $E\{XY\}=E\{X\}E\{Y\}$ are called \textit{uncorrelated}. $E\{XY\}=0$ they are called \textit{orthogonal}.

$X,Y$ continuous. $p_{X,Y}$ the \textit{joint} pdf. \textit{independent} if $p_{X,Y}=p_X(x)p_Y(y)$. The \textit{conditional} pdf of $X$ given $Y=y$ is 
\begin{equation}
p_{X|Y}(x|Y=y)=\frac{p_{X,Y}(x,y)}{p_Y(y)}.
\end{equation}

A \textit{complex} random variable $Z=X+iY$ has pdf $p_Z$ defined as the joint pdf of the two real random variables $X$ and $Y$
\begin{equation}
p_Z(z):=p_{X,Y}(x,y).
\end{equation}
Mean
\begin{equation}
E\{Z\} := E\{X\}+ iE\{Y\}
\end{equation}
Variance 
\begin{equation}
\mbox{Var}\{Z\} := E\{ | Z - E\{Z\} |^2 \} = E\{|Z|^2\}-|E\{Z\}|^2.
\end{equation}

\subsubsection{Stochastic Processes}
Let $T\subset \Rbb$. A \textit{stochastic} process $X(t)$ is a family of random variables labelled by the numbers $t\in T$, all defined on the same probability space $(\Xcal, \Ocal, P)$. For each finite subset ${t_1, \ldots, t_n} \subset T$ the random variables $ X(t_1), \ldots, X(t_n)$ have a joint $n$-dimensional cumulative distribution function $F$ defined by
\begin{equation}
F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)=P(x(t_1)\leq x_1, \ldots x(t_n)\leq t_n).
\end{equation}
When $T$ is a set of discrete points the stochastic process is called a \textit{random sequence}, and if the variable $t$ is time it's called a \textit{time series}. 

The stochastic process is \textit{Gaussian} if the cdf $F_{t_1, \ldots, t_n}$ is Gaussian for any $n$ and any $t_1, \ldots, t_n \in T$. \textit{Stationary} if for all finite dimensional cdfs and all $\tau$
\begin{equation}
F_{t_1+\tau, \ldots, t_n+\tau}(x_1,\ldots,x_n)=F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)
\end{equation}

The \textit{moments} of the probability distribution are 
\begin{equation}
\mu_{m_1 \ldots m_n} := E\{X(t_1)^{m_1}\cdots X(t_n)^{m_n} =\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty} x_1^{m_1}\cdots x_n^{m_n} \d F_{t_1, \ldots, t_n}(x_1,\ldots,x_n).
\end{equation}


This is an $n$-fold \textit{Stieltjes integral}. When $F$ is differentiable, it can be written in terms of the joint pdf 
\begin{equation}
f_{t_1\ldots t_n}(x_1\ldots x_n) = \frac{\pd^n \d F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)}{\pd x_1 \ldots \pd x_n} 
\end{equation}
and the above integral can be written 
\begin{equation}
\mu_{m_1 \ldots m_n} = \int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}x_1^{m_1}\cdots x_n^{m_n} f_{t_1\ldots t_n}(x_1\ldots x_n) \d x_1 \ldots \d x_n.
\end{equation}


 The first order moment is called the \textit{mean} value of the stochastic process and is denoted by $m(t) := E\{X(t)\} =\mu_1$, if the process is stationary this is constant. The second order moment is called the \textit{autocorrelation function} and is denoted by $K(t,s) := E\{X(t)X(s)\}$, if the process is stationary it depends only on the time difference $K(t,s)=R(t-s)$ for some function $R$ s.t. $R(0)>0$, $R(-\tau)=R(\tau)$ and $|R(\tau)|<R(0)$.

For a complex stationary process, $R(t):=E\{X(t+\tau)X(t)^*\}$, and $R(-\tau)=R(\tau)^*$. Consider a complex stationary stochastic process 
\begin{equation}
X(t)=X_0 U(t),
\end{equation}
where $X_0$ is a random variable with $E\{X_0\}=0$, and $U:T\rightarrow \Cbb$ with $\int_{-\infty}^{\infty} U(t)dt =0$. Stationarity gives $R(\tau)=E\{X(t+\tau)X(t)^*\}=E\{X_0 U(t+\tau) X_0^*U(t)^*\}=E\{|X_0|^2\}U(t+\tau)U(t)^*$ is independent of $t$. If we take $\tau=0$, this means $|U(t)|^2=r^2$, a constant, thus $U(t)=r\exp (i\phi(t))$. For non-zero $\tau$, $r^2 \exp (i\phi(t+\tau))\exp (-i\phi(t))=r^2 \exp (i(\phi(t+\tau)-\phi(t))$ is independent of $t$, implying $\phi'(t)=\omega$ for some constant $\omega$, and therefore $\phi=\omega t + \theta$. Therefore, $X(t)=X_0 \exp(i\omega t)$, where the $r$ and $\exp(\theta)$ have been ``absorbed'' into the $X_0$. The autocorrelation function $R(\tau)=E\{|X_0|^2\}\exp(i\omega \tau )=b_0 \exp(i\omega \tau )$, where $b_0=E\{|X_0|^2\}=E\{|X_0-E\{X_0\}|^2\}$ is the variance of $X_0$.

A more general complex stochastic process can be written as a superposition 
\begin{equation}
X(t)=\sum_{k=1}^{\infty}X_k \exp(i\omega_k t),
\end{equation}
where for each complex r.v. $X_k$, $E\{X_k\}=0$ and $E\{X_k X_l^* \}=0$ for $k\neq l$. The autocorrelation function is then 
\begin{equation}
R(\tau)=\sum_{k=1}^{\infty} b_k \exp(i\omega_k \tau),
\end{equation}
with each variance $b_k < \infty$. Such a stationary process is called a \textit{process with discrete spectrum}, where the set $\{\omega_k\}$ is the \textit{spectrum}.

Any complex stationary stochastic process can be obtained as the limit of a sequence of processes with discrete spectra. 

\paragraph{Cram\`er Representation Theorem} Let $X(t)$, $-\infty < t < \infty$, be a zero-mean stochastically continuous complex stationary process. Then there exists an orthogonal process $Z(\omega)$ such that for all $t$, $X(t)$ may be written in the form,
\begin{equation}
X(t)=\int^{\infty}_{-\infty}\exp(i\omega t) \d Z(\omega),
\end{equation}
the integral being defined in the mean-square sense. The process $Z(\omega)$ has the following properties:
\begin{enumerate}
\item $E\{\d Z(\omega) \} = 0 \forall \omega$
\item $E\{|\d Z(\omega)|^2 \} = \d F(\omega) \forall \omega$, where $F(\omega)$ is the (non-normalised) integrated spectrum of $X(t)$
\item for any two distinct frequencies $\omega, \omega'$
\begin{equation}
\mbox{Cov}\{\d Z(\omega), \d Z(\omega') \} = E\{ \d Z(\omega) \d Z(\omega')^* \} =0
\end{equation} 
\end{enumerate}

\paragraph{Wiener-Khinchin Theorem} A necessary and sufficient condition for $\rho(\tau)$ to be the autocorrelation function of a stochastically continuous stationary process $X(t)$ is that there exists a function $F(\omega)$ having the properties of a cdf on $(-\infty, \infty)$ such that for all $\tau$, $\rho(\tau)$ may be expressed in the form
\begin{equation}
\rho(\tau)=\int^{\infty}_{-\infty}\exp(i\omega \tau) \d F(\omega)
\end{equation}

If $F$ differentiable everywhere, 
\begin{equation}
\d F(\omega)=S(\omega) \d \omega
\end{equation}
$S$ is called the \textit{two-sided spectral density} of the stationary stochastic process. In this case, the correlation between the Fourier components are 
\begin{equation}
E\{\tilde{X}(\omega) \tilde{X}(\omega ' )^* \} = S(\omega) \delta(\omega ' -\omega)
\end{equation}

In the discrete-time case, a time series $X_k=X_0 U(k)$, $k \in \Zbb$, is stationary iff it is of the form
\begin{equation}
X_k = X_0 \exp(i \omega_0 k).
\end{equation}
The autocorrelation function is $R(l)=E\{X_k X_{k+l} \}$. The Cram\`er representation and Wiener-Khinchen theorems become 
\begin{equation}
\begin{array}{rcl}
x_k & = & \int_{-\pi}^{\pi} \exp(i\omega k ) \d Z(\omega) \\
R(l) & = & \int_{-\pi}^{\pi} \exp(i\omega l ) \d F(\omega) 
\end{array}
\end{equation}
If $F$ is differentiable with derivative $S$, we can write
\begin{equation}
R(l)=\int_{-\pi}^{\pi} \exp(i \omega l) S(\omega) \d \omega,
\end{equation}
which can be solved to find 
\begin{equation}
S(\omega)=\frac{1}{2\pi}\sum_{l=-\infty}^{\infty} \exp(-i\omega l) R(l).
\end{equation}


\subsubsection{Hypothesis Testing}


\paragraph{Bayesian Approach}

Assign \textit{costs} $C_{ij}$, $i,j=0,1$ incurred by choosing hypothesis $H_i$ when $H_j$ is true. We say these costs are \textit{uniform} if $C_{ij}=0$ when $i=j$ and $C_{ij}=1$ when $i \neq j$. 

\textit{Conditional Risk}$R_j$ of a decision rule $\delta$ is 
\begin{equation}
\label{eq:conditionalrisk}
R_j(\delta)=C_{0j}P_j(\Rcal)+C_{1j}P_1(\Rcal')
\end{equation}
where $P_j(\Rcal)=P(X=x\in \Rcal | H_j)= \int_{\Rcal} p_j(x)dx$ is the probability distribution of the data wehen hypothesis $H_j$ is true.

\textit{A priori probabilities/ priors} $\pi_0$, $\pi_1=1-\pi_0$ of hypotheses $H_0$ and $H_1$.

\textit{Bayes' Risk} average cost of rule $\delta$:
\begin{equation}
\label{eq:BayesRisk}
\begin{array}{rcl}
r(\delta)&=&\pi_0 R_0(\delta) + \pi_1 R_1(\delta)\\
&=& \pi_0 C_{00} + \pi_1 \\
\end{array}
\end{equation}

\subsubsection{Matched filter}
\begin{equation}
x(t)=s(t)+n(t)
\end{equation}
\begin{itemize}
\item $x=$data
\item $s=$signal, deterministic
\item $n=$noise, additive, continuous Gaussian stochastic process, zero mean. 
\end{itemize}
Consider it to be continuous for theory, obtain results by suitable sampling of continuous expressions.

Observed on interval $[0,T_0]$. Can be shown that log-likelihood is (the Cameron-Martin formula)
\begin{equation}
\label{eq:Cameron-Martin}
\ln \Lambda [x] =  \int_0^{T_0}q(t)x(t)\d t - \frac{1}{2}\int_0^{T_0}q(t)s(t)\d t
\end{equation}
where $q$ is the soln to
\begin{equation}
s(t) = \int_0^{T_0} K(t,t')q(t')\d t'.
\end{equation}
and $K$ is the autocorrelation function of the noise. This means the likelihood ratio test is equivalent to comparing 
\begin{equation}
G:=\int_0^{T_0}q(t)x(t)\d t
\end{equation}
to a threshold $G_0$. Hypothesis test expectations:
\begin{equation}
E_0\{G\}=0,\\
E_1\{G\} = \int_0^{T_0}q(t)s(t)\d t.
\end{equation}
\begin{equation}
\begin{array}{rcl}
Var\{G\}&=& \int_0^{T_0}\int_0^{T_0}q(t_1)q(t_2)E\{n(t_1)n(t_2)\}\d t_1  \d t_2 \\
&=&\int_0^{T_0}q(t_1)q(t_2)K(t_1,t_2)\d t_1  \d t_2\\
&=&int_0^{T_0}q(t)s(t)\d t = \rho^2
\end{array}
\end{equation}
$\rho$ is the \textit{signal to noise ratio}.

Since data $x$ is Gaussian, and $G$ is linear in $x$, pdf's when signal is absent or present are
\begin{equation}
p_0(G)=\frac{1}{\sqrt{2\pi\rho^2}}\exp(-\frac{G^2}{2\rho^2}
p_1(G)=\frac{1}{\sqrt{2\pi\rho^2}}\exp(-\frac{(G-\rho^2)^2}{2\rho^2}
\end{equation}


%\end{document}
