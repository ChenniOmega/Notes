\input{ThesisPreamble}
\begin{document}

\chapter{Data analysis}           
\section{Introduction}     
For a known waveform the most efficient data analysis tool is matched filtering. But often the waveforms of GW are poorly known, limiting the matched filter's applicability. 

A linear time frequency transform correlates the signal with a family of waveforms that are well concentrated in time and space. These waveforms are called \textit{time frequency atoms}.

\section{Fourier Transforms}
\[\hat{f}(\omega)=\int^{+\infty}_{-\infty}f(t)e{-i\omega t}dt \]

\section{Windowed Fourier Transforms}
Gabor atoms 
\[ g_{u,\xi} (t)=g(t-u)e^{i\xi t} \]
When $g$ is Gaussian, best localisation and the atoms are called \textit{Gabor functions.} Fourier transform of Gabor atom
\[ \hat{g}_{u,\xi} (\omega) =\hat{g}(\omega-\xi) e^{-iu(\omega-\xi)} \]
Windowed Fourier transform:
\[ Sf(u,\xi)=\int^{+\infty}_{-\infty}f(t)g*_{u,\xi}(t)dt=\int^{+\infty}_{-\infty}f(t)g(t-u)e^{-i\xi t}dt \]
Also can be written as frequency integral
\[ Sf(u,\xi)=\frac{1}{2\pi}\int^{+\infty}_{-\infty}\hat{f}(\omega)\hat{g}^{*}_{u,\xi}(\omega)d\omega \]

\section{wavelet transforms}
Wavelet transforms are described in terms of families of basis functions $\psi_{a,b}(x)$ which are dilations and translations of a real function $\psi(x)$ of zero average:
\[ \psi_{a,b}(t)=\frac{1}{\sqrt{|a|}}\psi\left(\frac{t-b}{a}\right) \]
A wavelet transform of a function $f$ has a different time frequency resolution to a windowed Fourier transfrom. It is computed by correlating $f$ with a wavelet atom 
\[Wf(a,b)=\int^{+\infty}_{-\infty}f(t)\psi^{*}_{a,b}(t)dt=\frac{1}{2\pi}\int^{+\infty}_{-\infty} \hat{f}(\omega)\hat{\psi}^{*}_{a,b}(\omega) d\omega \]

\section{Notes from "method for detection and reconstruction of gravitational wave transients with networks of advanced detectors}
\subsection{Low latency search}
Designed to identify potentially significant candidates in real time (within minutes). Streaming? Promptly share sky location with partner telescopes to search for EM counterparts eg Gamma ray burst and afterglow or kilonova which will fade within seconds to days. 

\subsection{Archive analysis}
Establish significance of event and identify progenitors. Slower and more detailed. 

\subsection{Inverse problem for data from a network of detectors}
Network of $K$ detectors. Time series data from the detectors is transformed with the Wilson-Debauchies-Meyer (WDM) transform so that each detector $k$ presents data $x_k[i]$ in a general time-frequency (TF) domain, where $i$ is the data sampling (TF pixel) index. 

Detector noise assumed to be Gaussian, described by WDM power spectral density $S_k[i]$ estimated for every data sample. The noise-scaled (whitened) data is defined as $\omega_k[i] = x_k[i]/\sqrt{S_k[i]}$

TF series combined to obtain energy TF maps $E[i]=\sum_k \omega^2_k [i]$ where the $E[i]$ are maximised over all possible time-of-flight delays in the network (i think this means it's lining up the signals based on the travel time of GW from one detector to another. Obviously this depends on where in the sky the GW is coming from). The energy maps are are used to identify TF areas with excess energy above baseline detector noise using an appropriate clustering algorithm. The inverse problem is to extract the signal waveform, polarisation and sky location. (Note that this slightly differs from my definition of the inverse problem which is to determine the parameters from the waveform. Many steps to the inverse problem). 

Data vector $\bfx [i]={x_1[i], \ldots , x_K[i]}$ recorded by detector at time of a GW signal $\bfh [i] = [h_+[i], h_{\times}[i] ]$ coming from sky location $\theta$ and $\phi$ is a superposition of network response $\Fcal \bfh[i]$ and noise $\bfn[\bfi]$:
\[ \bfx [i] = \Fcal \bfh[i]+\bfn[\bfi] \]
where $\Fcal$ is the network antenna pattern matrix:
\[ \Fcal = \left[ \begin{array}{cc} 
				F_{1+}(\theta, \phi) & F_{1\times}(\theta, \phi) \\
				\ldots				& \ldots	\\
				F_{K+}(\theta, \phi) & F_{K \times}(\theta, \phi) \end{array} \right] \]

(Antenna patterns often also include a transformation by polarisation angle $\Psi$, but this is equivalent to a rotation of the wave frame and can be included in the definition of $\bfh$).

BX-LBTN-KYW2-9V7B-9K2D
\subsection{Matched Filtering}

from \cite{PhysRevD.82.044025}
Searches for GWs from CBC events employ matched filtering as the first step. 

"whitening": Optimal strategy, weight detector output and template wavefor by inverse of amplitude spectral density of detector noise 

discretely sampled time series
whitened data: $\overrightarrow{s}={s_i}$
whitened template waveform: $\overrightarrow{h}_\alpha = h_{\alpha i}$
noise: $\overrightarrow{n}={n_i}$ zero-mean $<n_i>=0$, unit-variance Gaussian random process $<n_i,n_j>=\delta_{ij}$


output of matched filter is given by the vector inner product 
\begin{equation}
\rho_\alpha = \overrightarrow{h}_\alpha^*\cdot \overrightarrow{s}
\end{equation}

search over phase by using complex-valued templates where $\mathfrak{R}\overrightarrow{h}_\alpha$ contains the cosine like phase and $\mathfrak{I}\overrightarrow{h}_\alpha$ contains the sine-like phase. 

\section{Jaranowski et.al}

\subsection{Random Variables}
Def: Probability space $(\Xcal, \Ocal, \Pcal)$  
\begin{itemize}
\item $\Xcal$ is a sample space, usually an $n$-dimensional Euclidean space $\Rbb^n$ or a discrete set $\Gamma=\{\gamma_1,\gamma_2, \ldots\}$
\item $\Ocal$ is a $\sigma$-algebra of subsets of $\Xcal$ called observation events, these are the set of values that data can take. 
$\sigma$-algebra:
\begin{enumerate}
\item $\Xcal \in \Ocal$
\item if $A \in \Ocal$, then $A^c := \Xcal - A \in \Ocal$
\item for any countable family $A_k$ of members of $Ocal$, their union is in $\Ocal$
\end{enumerate}
Together these imply that the intersection of a finite or countable family of members of $\Ocal$ belong to $\Ocal$.
\begin{itemize}
\item if $\Xcal=\Rbb^n$, consider open intervals 
\begin{equation}
\label{eq:openInts}
\{ \bfy = (y_1,\ldots, y_n)\in \Rbb^n : a_1 < y_1 < b_1, \ldots a_n < y_n < b_n \}
\end{equation}
where $a_i$ and $b_i$ are real numbers. Then usually $\Ocal$ is the smalles $\sigma$-algebra containing all such intervals. This is known as the \textit{Borel} $\sigma$-algebra and is denoted by $\Bcal^n$. It can also be defined in terms of closed intervals.
\item if $\Xcal$ is a discrete set $\Gamma$, the $\sigma$-algebra $\Ocal$ is the set of all subsets of $\Gamma$. This is called the \textit{power set} of $\Gamma$ and is denoted by $2^{\Gamma}$.
\end{itemize}
\item $\Pcal$ is a probability measure on $\Ocal$. This is a function $\Pcal:\Xcal \rightarrow [0,1]$ such that $\Pcal(\Xcal)=1$. $P(A)=\int_A \d \Pcal(A)$, $A\in \Ocal$ is the probability that $A$ occurs. For probability spaces $(\Rbb^n, \Bcal^n, \Pcal)$ usually there exists a \textit{probability density function} $p: \Rbb^n \rightarrow [0,\infty) $ such that 
\begin{equation}
P(A)=\int_A p(\xbf) \d \xbf
\end{equation}
where $\xbf$ are some coordinates on $\Rbb^n$. 
\end{itemize}

A \textit{random variable} $X$ is a real-valued function $X:\Xcal \rightarrow \Rbb$. The sets $\{ \xi \in \Xcal : X(\xi) \leq x \} \in \Ocal \forall x \in \Rbb$. If $f:\Rbb \rightarrow \Rbb$ is a Borel measurable function, then $Y=f \circ X$ is also a random variable. ($f$ Borel measurable means $f^{-1}(A)\in\Bcal \forall A \in \Bcal$)

The \textit{cumulative distribution function} of $X$ is 
\begin{equation}
\label{eq:cdf}
P_X(x) := P(X\leq x).
\end{equation} 
It is non-decreasing and right-continuous.

A \textit{continuous} random function is one with a continuous cumulative distribution function. If it is differentiable, then the probability density  function is 
\begin{equation}
\label{eq:pdf}
p_X(x)=\frac{\d P_X(x)}{\d x}.
\end{equation}
This is always non-negative as the cdf is non-decreasing. It is also normalised 
\begin{equation}
\label{eq:pdfNormalised}
\int_{-\infty}^{\infty} p_X(x) \d x =1
\end{equation}

The \textit{expectation value} or \textit{mean} of a continuous random variable is 
\begin{equation}
\label{eq:expectation}
E\{X\} := \int_{\Xcal} x \d P(x) :=\mu
\end{equation}
\begin{enumerate}
 \item assume X is integrable: $E\{|X|\} < \infty$
 \item $E\{ X+Y \} = E\{X\} + E\{Y\}$
 \item $E\{cX\}=cE\{X\}$, $c\in \Rbb$.
\end{enumerate}

The \textit{variance} of the random variable $X$ is 
\begin{equation}
\mbox{Var} \{ X \} \equiv \sigma^2_X := E\{(X-E\{X\})^2\},
\end{equation}
which we will always assume to be finite.

The \textit{covariance} between two random variables $X$ and $Y$ defined on the same probability space is 
\begin{equation}
\mbox{Cov}\{XY\} := E\{(X-E\{X\})(Y-E\{Y\})\} = E\{XY\}-E\{X\}E\{Y\}.
\end{equation}
Random variables for which $E\{XY\}=E\{X\}E\{Y\}$ are called \textit{uncorrelated}. $E\{XY\}=0$ they are called \textit{orthogonal}.

$X,Y$ continuous. $p_{X,Y}$ the \textit{joint} pdf. \textit{independent} if $p_{X,Y}=p_X(x)p_Y(y)$. The \textit{conditional} pdf of $X$ given $Y=y$ is 
\begin{equation}
p_{X|Y}(x|Y=y)=\frac{p_{X,Y}(x,y)}{p_Y(y)}.
\end{equation}

A \textit{complex} random variable $Z=X+iY$ has pdf $p_Z$ defined as the joint pdf of the two real random variables $X$ and $Y$
\begin{equation}
p_Z(z):=p_{X,Y}(x,y).
\end{equation}
Mean
\begin{equation}
E\{Z\} := E\{X\}+ iE\{Y\}
\end{equation}
Variance 
\begin{equation}
\mbox{Var}\{Z\} := E\{ | Z - E\{Z\} |^2 \} = E\{|Z|^2\}-|E\{Z\}|^2.
\end{equation}

\subsection{Stochastic Processes}
Let $T\subset \Rbb$. A \textit{stochastic} process $X(t)$ is a family of random variables labelled by the numbers $t\in T$, all defined on the same probability space $(\Xcal, \Ocal, P)$. For each finite subset ${t_1, \ldots, t_n} \subset T$ the random variables $ X(t_1), \ldots, X(t_n)$ have a joint $n$-dimensional cumulative distribution function $F$ defined by
\begin{equation}
F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)=P(x(t_1)\leq x_1, \ldots x(t_n)\leq t_n).
\end{equation}
When $T$ is a set of discrete points the stochastic process is called a \textit{random sequence}, and if the variable $t$ is time it's called a \textit{time series}. 

The stochastic process is \textit{Gaussian} if the cdf $F_{t_1, \ldots, t_n}$ is Gaussian for any $n$ and any $t_1, \ldots, t_n \in T$. \textit{Stationary} if for all finite dimensional cdfs and all $\tau$
\begin{equation}
F_{t_1+\tau, \ldots, t_n+\tau}(x_1,\ldots,x_n)=F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)
\end{equation}

The \textit{moments} of the probability distribution are 
\begin{equation}
\mu_{m_1 \ldots m_n} := E\{X(t_1)^{m_1}\cdots X(t_n)^{m_n} =\int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty} x_1^{m_1}\cdots x_n^{m_n} \d F_{t_1, \ldots, t_n}(x_1,\ldots,x_n).
\end{equation}


This is an $n$-fold \textit{Stieltjes integral}. When $F$ is differentiable, it can be written in terms of the joint pdf 
\begin{equation}
f_{t_1\ldots t_n}(x_1\ldots x_n) = \frac{\pd^n \d F_{t_1, \ldots, t_n}(x_1,\ldots,x_n)}{\pd x_1 \ldots \pd x_n} 
\end{equation}
and the above integral can be written 
\begin{equation}
\mu_{m_1 \ldots m_n} = \int_{-\infty}^{\infty}\cdots \int_{-\infty}^{\infty}x_1^{m_1}\cdots x_n^{m_n} f_{t_1\ldots t_n}(x_1\ldots x_n) \d x_1 \ldots \d x_n.
\end{equation}


 The first order moment is called the \textit{mean} value of the stochastic process and is denoted by $m(t) := E\{X(t)\} =\mu_1$, if the process is stationary this is constant. The second order moment is called the \textit{autocorrelation function} and is denoted by $K(t,s) := E\{X(t)X(s)\}$, if the process is stationary it depends only on the time difference $K(t,s)=R(t-s)$ for some function $R$ s.t. $R(0)>0$, $R(-\tau)=R(\tau)$ and $|R(\tau)|<R(0)$.

For a complex stationary process, $R(t):=E\{X(t+\tau)X(t)^*\}$, and $R(-\tau)=R(\tau)^*$. Consider a complex stationary stochastic process 
\begin{equation}
X(t)=X_0 U(t),
\end{equation}
where $X_0$ is a random variable with $E\{X_0\}=0$, and $U:T\rightarrow \Cbb$ with $\int_{-\infty}^{\infty} U(t)dt =0$. Stationarity gives $R(\tau)=E\{X(t+\tau)X(t)^*\}=E\{X_0 U(t+\tau) X_0^*U(t)^*\}=E\{|X_0|^2\}U(t+\tau)U(t)^*$ is independent of $t$. If we take $\tau=0$, this means $|U(t)|^2=r^2$, a constant, thus $U(t)=r\exp (i\phi(t))$. For non-zero $\tau$, $r^2 \exp (i\phi(t+\tau))\exp (-i\phi(t))=r^2 \exp (i(\phi(t+\tau)-\phi(t))$ is independent of $t$, implying $\phi'(t)=\omega$ for some constant $\omega$, and therefore $\phi=\omega t + \theta$. Therefore, $X(t)=X_0 \exp(i\omega t)$, where the $r$ and $\exp(\theta)$ have been ``absorbed'' into the $X_0$. The autocorrelation function $R(\tau)=E\{|X_0|^2\}\exp(i\omega \tau )=b_0 \exp(i\omega \tau )$, where $b_0=E\{|X_0|^2\}=E\{|X_0-E\{X_0\}|^2\}$ is the variance of $X_0$.

A more general complex stochastic process can be written as a superposition 
\begin{equation}
X(t)=\sum_{k=1}^{\infty}X_k \exp(i\omega_k t),
\end{equation}
where for each complex r.v. $X_k$, $E\{X_k\}=0$ and $E\{X_k X_l^* \}=0$ for $k\neq l$. The autocorrelation function is then 
\begin{equation}
R(\tau)=\sum_{k=1}^{\infty} b_k \exp(i\omega_k \tau),
\end{equation}
with each variance $b_k < \infty$. Such a stationary process is called a \textit{process with discrete spectrum}, where the set $\{\omega_k\}$ is the \textit{spectrum}.

Any complex stationary stochastic process can be obtained as the limit of a sequence of processes with discrete spectra. 

\paragraph{Cram\`er Representation Theorem} Let $X(t)$, $-\infty < t < \infty$, be a zero-mean stochastically continuous complex stationary process. Then there exists an orthogonal process $Z(\omega)$ such that for all $t$, $X(t)$ may be written in the form,
\begin{equation}
X(t)=\int^{\infty}_{-\infty}\exp(i\omega t) \d Z(\omega),
\end{equation}
the integral being defined in the mean-square sense. The process $Z(\omega)$ has the following properties:
\begin{enumerate}
\item $E\{\d Z(\omega) \} = 0 \forall \omega$
\item $E\{|\d Z(\omega)|^2 \} = \d F(\omega) \forall \omega$, where $F(\omega)$ is the (non-normalised) integrated spectrum of $X(t)$
\item for any two distinct frequencies $\omega, \omega'$
\begin{equation}
\mbox{Cov}\{\d Z(\omega), \d Z(\omega') \} = E\{ \d Z(\omega) \d Z(\omega')^* \} =0
\end{equation} 
\end{enumerate}

\paragraph{Wiener-Khinchin Theorem} A necessary and sufficient condition for $\rho(\tau)$ to be the autocorrelation function of a stochastically continuous stationary process $X(t)$ is that there exists a function $F(\omega)$ having the properties of a cdf on $(-\infty, \infty)$ such that for all $\tau$, $\rho(\tau)$ may be expressed in the form
\begin{equation}
\rho(\tau)=\int^{\infty}_{-\infty}\exp(i\omega \tau) \d F(\omega)
\end{equation}

If $F$ differentiable everywhere, 
\begin{equation}
\d F(\omega)=S(\omega) \d \omega
\end{equation}
$S$ is called the \textit{two-sided spectral density} of the stationary stochastic process. In this case, the correlation between the Fourier components are 
\begin{equation}
E\{\tilde{X}(\omega) \tilde{X}(\omega ' )^* \} = S(\omega) \delta(\omega ' -\omega)
\end{equation}

In the discrete-time case, a time series $X_k=X_0 U(k)$, $k \in \Zbb$, is stationary iff it is of the form
\begin{equation}
X_k = X_0 \exp(i \omega_0 k).
\end{equation}
The autocorrelation function is $R(l)=E\{X_k X_{k+l} \}$. The Cram\`er representation and Wiener-Khinchen theorems become 
\begin{equation}
\begin{array}{rcl}
x_k & = & \int_{-\pi}^{\pi} \exp(i\omega k ) \d Z(\omega) \\
R(l) & = & \int_{-\pi}^{\pi} \exp(i\omega l ) \d F(\omega) 
\end{array}
\end{equation}
If $F$ is differentiable with derivative $S$, we can write
\begin{equation}
R(l)=\int_{-\pi}^{\pi} \exp(i \omega l) S(\omega) \d \omega,
\end{equation}
which can be solved to find 
\begin{equation}
S(\omega)=\frac{1}{2\pi}\sum_{l=-\infty}^{\infty} \exp(-i\omega l) R(l).
\end{equation}


\subsection{Hypothesis Testing}
\begin{itemize}
\item \textit{Null hypothesis} $H_0$: Signal is absent. Data have pdf $p_0(x)$
\item \textit{Alternative hypothesis} $H_1$: signal is present. Data have pdf $p_1(x)$
\item \textit{Likelihood ratio} $\Lambda(x) := \frac{p_1(x)}{p_0(x)}$
\item \textit{Hypothesis test/ decision rule} $\delta$: partitioun of observation set $\Xcal$ into two subsets $\Rcal$ and $\Rcal'=\Xcal-\Rcal$. If the data are in $\Rcal$ we accept the null hypothesis, otherwise we reject it. $\int_{\Rcal} p_i(x) dx+ \int_{\Rcal'} p_i(x) dx=1$ for $i=0,1$.
\item \textit{Type I error:} choosing $H_1$ when $H_0$ is true (false alarm), has probability 
\begin{equation}
P_D := \int_{\Rcal} p_1(x) dx
\end{equation}
\item \textit{Type II error:}  choosing $H_0$ when $H_1$ is true (false dismissal), has probability 
\begin{equation}
P_F := \int_{\Rcal'} p_0(x) dx
\end{equation}
\end{itemize}
Problem: find a test which is optimal.

\subsubsection{Bayesian Approach}

Assign \textit{costs} $C_{ij}$, $i,j=0,1$ incurred by choosing hypothesis $H_i$ when $H_j$ is true. We say these costs are \textit{uniform} if $C_{ij}=0$ when $i=j$ and $C_{ij}=1$ when $i \neq j$. 

\textit{Conditional Risk}$R_j$ of a decision rule $\delta$ is 
\begin{equation}
\label{eq:conditionalrisk}
R_j(\delta)=C_{0j}P_j(\Rcal)+C_{1j}P_1(\Rcal')
\end{equation}
where $P_j(\Rcal)=P(X=x\in \Rcal | H_j)= \int_{\Rcal} p_j(x)dx$ is the probability distribution of the data when hypothesis $H_j$ is true.

\textit{A priori probabilities/ priors} $\pi_0$, $\pi_1=1-\pi_0$ of hypotheses $H_0$ and $H_1$.

\textit{Bayes' Risk} average cost of rule $\delta$:
\begin{equation}\label{eq: BayesRisk}\begin{array}{rcl}
r(\delta)&=&\pi_0 R_0(\delta) + \pi_1 R_1(\delta) \\
&=& \pi_0 C_{00} + \pi_1
\end{array}
\end{equation}

\text{Bayes' Rule} is a decision rule that minimises the Bayes' Risk
\begin{equation}
\label{eq:BayesRule}
\delta^* = \mbox{argmin} r(\delta). 
\end{equation}
We can expand (\ref{eq: BayesRisk}) to find 
\begin{equation}
r(\delta)=\pi_0 C_{00} + \pi_1 C_{01} + \int_{\Rcal'} \pi_0(C_{10}-C_{00}) + \pi_1(C_{11}-C_{01}) dx.
\end{equation}
It is clear an optimal decision rule is one which assigns $x \in \Rcal'$ when the term inside the integral above is negative. Assuming the costs of errors are greater than the costs of choosing the correct hypothesis, this means the decision rule is 
\begin{equation}
\delta^*: x \in \Rcal' \mbox{ iff } \Lambda(x) < \frac{\pi_0}{\pi_1}\frac{C_{10}-C_{00}}{C_{01}-C_{11}}.
\end{equation}
That is, the decision rule is equivalent to comparing the likelihood ratio to a threshold $\lambda := \frac{\pi_0}{\pi_1}\frac{C_{10}-C_{00}}{C_{01}-C_{11}}$.

Example:
\begin{equation}
\begin{array}{rcl}
H_0: X &=& N \sim \Ncal(0,\sigma^2) \\
H_1: X &=& N + \mu \sim \Ncal(\mu,\sigma^2)\\
\rightarrow \Lambda(x) &=& \exp(\frac{x \mu}{\sigma^2} - \frac{\mu^2}{\sigma^2})
\end{array}
\end{equation}
Cases:
\begin{enumerate}
\item equal priors $\pi_0 = \pi_1 =\frac{1}{2}$, uniform costs, $\lambda=1$
\item High risk, such as false announcement of detection of extra-terrestrial life. $\pi_0 >> \pi_1$, $C_{10}>>C_{01}$. E.g. $\frac{\pi_0}{\pi_1}=10$, $\frac{C_{10}}{C_{01}}=10$, $\rightarrow \lambda = 100$. 
\item Low risk, such as wartime false warning of air raid. $\pi_1 >> \pi_0$, $C_{01}>>C_{10}$. E.g. $\frac{\pi_0}{\pi_1}=0.1$, $\frac{C_{10}}{C_{01}}=0.1$, $\rightarrow \lambda = 10^{-2}$. 
\end{enumerate}

\subsubsection{Minimax Approach}
Sometimes we cannot assign priors, and therefore cannot find the Bayes' solution. In the minimax approach we redefine risk in terms of an unknown prior, $r(\pi_0, \delta)$, a convex and continuous function of $\pi_0$. The \textit{least favourable prior} $\pi_L = \argmax_{\pi_0} r(\pi_0,\delta)$. Then the \textit{minimax rule} is the decision rule $\delta_{\pi_L}$that minimises the risk with respect to $\pi_L$, that is:
\begin{equation}
\label{eq:minimaxRule}
\delta_{\pi_L}=\argmin_{\delta} \max_{0\leq \pi_0 \leq 1} r(\pi_0, \delta)
\end{equation}

\subsubsection{Neyman-Pearson Approach}
Sometime there's no obvious way to set costs, either. Define
\begin{itemize}
\item \textit{Significance of test} is the probability of of a Type I error/ false alarm $P(x\in R'|H_0)=P_F:=\alpha$
\item \textit{Power of test}$=1-P(x\in R|H_1)=1-P_D$
\end{itemize}
Neyman-Pearson design: maximise power of test (i.e. probability of detection) subject to chosen significance of test $\alpha$ (i.e. subject to constraining false alarm probability). 

Using Lagrange multiplier approach, we are seeking to minimise 
\begin{equation}
C(\delta) = (1-P_D)+\lambda (P_F - \alpha).
\end{equation}
This can be expanded as follows:
\begin{equation}
\begin{array}{rcl}
C(\delta) &=& \int_{\Rcal} p_1 (x) dx + \lambda \left(\int_{\Rcal'} p_0(x)dx-\alpha \right) \\
&=& \int_{\Rcal} p_1 (x) dx + \lambda \left(1- \int_{\Rcal}  p_0(x)dx
 -\alpha \right) \\
&=& \lambda (1-\alpha)+\int_{\Rcal} p_1 (x) - \lambda p_0(x)dx.
\end{array}
\end{equation}
As in the Bayesian case, minimising $C(\delta)$ becomes choosing $x \in \Rcal$ such that the term inside the integral is negative. That is:
\begin{equation}
\delta: x \in \Rcal \mbox{ iff } p_1 (x) - \lambda p_0(x) < 0,
\end{equation}
which can be rearranged into another likelihood ratio test:
\begin{equation}
\delta: x \in \Rcal \mbox{ iff } \Lambda(x) = \frac{p_1 (x)}{ p_0(x)} < \lambda,
\end{equation}

Thus all three hypothesis testing methods we have looked at result in comparing the likelihood ratio to a threshold, but each determine the threshold in a different way.

\subsection{Matched filter}
\begin{equation}
x(t)=s(t)+n(t)
\end{equation}
\begin{itemize}
\item $x=$data
\item $s=$signal, deterministic
\item $n=$noise, additive, continuous Gaussian stochastic process, zero mean. 
\end{itemize}
Consider it to be continuous for theory, obtain results by suitable sampling of continuous expressions.

Observed on interval $[0,T_0]$. Can be shown that log-likelihood is (the Cameron-Martin formula)
\begin{equation}
\label{eq:Cameron-Martin}
\ln \Lambda [x] =  \int_0^{T_0}q(t)x(t)\d t - \frac{1}{2}\int_0^{T_0}q(t)s(t)\d t
\end{equation}
where $q$ is the soln to
\begin{equation}
s(t) = \int_0^{T_0} K(t,t')q(t')\d t'.
\end{equation}
and $K$ is the autocorrelation function of the noise. This means the likelihood ratio test is equivalent to comparing 
\begin{equation}
G:=\int_0^{T_0}q(t)x(t)\d t
\end{equation}
to a threshold $G_0$. Hypothesis test expectations:
\begin{equation}
E_0\{G\}=0,\\
E_1\{G\} = \int_0^{T_0}q(t)s(t)\d t.
\end{equation}
\begin{equation}
\begin{array}{rcl}
Var\{G\}&=& \int_0^{T_0}\int_0^{T_0}q(t_1)q(t_2)E\{n(t_1)n(t_2)\}\d t_1  \d t_2 \\
&=&\int_0^{T_0}q(t_1)q(t_2)K(t_1,t_2)\d t_1  \d t_2\\
&=& \int_0^{T_0}q(t)s(t)\d t = \rho^2
\end{array}
\end{equation}
$\rho$ is the \textit{signal to noise ratio}.

Since data $x$ is Gaussian, and $G$ is linear in $x$, pdf's when signal is absent or present are
\begin{equation}
\begin{array}{rcl}
p_0(G)&=&\frac{1}{\sqrt{2\pi\rho^2}}\exp(-\frac{G^2}{2\rho^2}) \\
p_1(G)&=&\frac{1}{\sqrt{2\pi\rho^2}}\exp(-\frac{(G-\rho^2)^2}{2\rho^2}
\end{array}
\end{equation}


\subsection{parameter estimation}


\end{document}
